[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Drew’s News",
    "section": "",
    "text": "Subscription\n\n\n\n\n\nSubscribe to the RSS feed for updates.\nIf you’d prefer to receive the blog as a newsletter, Feedrabbit is a good and simple choice (until I make something better).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuarto for an Academic Website\n\n\n\n\n\n\nwebsite\n\n\n\nI continue my long search for a way to generate a nicely formatted website with publication list based on adding publication information to a single source of truth without re-remembering how all the formatting works each time. \n\n\n\n\n\nMay 11, 2022\n\n\nDrew Dimmery\n\n\n\n\n\n\n\n\n\n\n\n\nUsing SoftBlock to Design an Experiment\n\n\n\n\n\n\nexperimental-design\n\n\ndemo\n\n\n\nIn this demo, I’m going to walk through an example experimental design using methods from our recent paper on Balanced Design.\n\n\n\n\n\nMay 10, 2022\n\n\nDrew Dimmery\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I am the Professor of Data Science for the Common Good at the Hertie School’s Data Science Lab.\n\nResearch Interests\n\nCausal Inference\nMachine Learning\nData Science\nExperimental Design\n\n\n\nEducation\n\n\nNew York University\nPhD in Political Methodology\nAdvisors: Cyrus Samii, Neal Beck, Josh Tucker\n\n\nNew York, NY\nGranted May 2016\n\n\n\nDissertation: Essays on Causal Inference and Machine Learning with Application to Nonprofits\nWinner of 2015 Williams Award for Best Dissertation Proposal in Political Methodology from the Society of Political Methodology\n\n\n\nUNC Chapel Hill\nB.A. in International and Area Studies with distinction\n\n\nChapel Hill, NC\nGranted June 2010\n\n\n\n\nExperience\n\n\nHertie School\n\n\nJanuary 2024 - present\n\n\nProfessor of Data Science for the Common Good\n\n\nUniversity of Vienna\n\n\nApril 2021 - December 2023\n\n\nScientific Coordinator\n\nSupervisor: Philipp Grohs\n\n\n\nFacebook Core Data Science\n\n\nSept 2016 - March 2021\n\n\nResearch Scientist\n\nPart of Eytan Bakshy’s Adaptive Experimentation team\nDeveloped statistical, machine learning and experimental methodology\nRan adaptive and contextual field experiments with a variety of product teams\nIntegrated advanced methodologies into a toolkit for scalable and automatic experimentation intended for optimization (Ax) - released at F8 2019)\nDeveloped scalable methods for robust observational causal inference as the technical lead of our “CausalML” initiative\n\n\n\nPrinceton University\n\n\nSeptember 2015 - May 2016\n\n\nPre-doctoral fellow\n\nSupervised by Kosuke Imai\n\n\n\nFacebook Core Data Science\n\n\nSummer 2015\n\n\nSummer Intern\n\nStatistical and Decision Science Team\nSupervised by Eytan Bakshy\n\n\n\nTeaching\n\nFacebook\n\n\nInternal datacamp class on designing and analyzing experiments\n\n\n2017-2019\n\n\n\n\nNYU Undergraduate\n\n\nTA for Power and Politics in America (under Jonathan Nagler)\n\n\nFall 2014\n\n\n\n\nTA for Games, Strategy and Politics (under Steven Brams)\n\n\nFall 2013\n\n\n\n\nNYU Graduate\n\n\nTA for Quantitative Methods II (under Nathaniel Beck)\n\n\nSpring 2015\n\n\n\n\nTA for Quantitative Methods II (under Cyrus Samii)\n\n\nSpring 2014\n\n\n\n\nHigh Performance Computing Talk for NYU Datalab\n\n\nFebruary 2014\n\n\n\n\nIntroduction to R for NYU Datalab\n\n\nJanuary 2013"
  },
  {
    "objectID": "posts/softblock-demo/index.html",
    "href": "posts/softblock-demo/index.html",
    "title": "Using SoftBlock to Design an Experiment",
    "section": "",
    "text": "In particular, I’m going to imagine that I’m designing an experiment in which I assign different treatments to particular precincts in North Carolina. In order to optimize power, of course, we want to make sure that our two test groups look as similar as possible in terms of prior voting patterns.\nThus, the steps in this design will be:\n\nCollect relevant historical data.\nDefine variables on which we wish to balance.\nAllocate treatment assignment using new methods.\nSimulate the power of hypothesis tests under the proposed design.\nFake some outcome data and analyze it for average and heterogeneous treatment effects."
  },
  {
    "objectID": "posts/softblock-demo/index.html#description",
    "href": "posts/softblock-demo/index.html#description",
    "title": "Using SoftBlock to Design an Experiment",
    "section": "Description",
    "text": "Description\nThe relevant API is a function with tidyverse semantics called assign_softblock (or assign_greedy_neighbors). These functions accept a vector of columns to be used in the design. The SoftBlock version additionally accepts two arguments, .s2 for the bandwidth of the RBF kernel to use in the construction of a similarity matrix as well as .neighbors which indicates the number of nearest neighbors to include in the graph on which to construct the spanning tree. These parameters don’t generally need to be modified."
  },
  {
    "objectID": "posts/softblock-demo/index.html#source-code",
    "href": "posts/softblock-demo/index.html#source-code",
    "title": "Using SoftBlock to Design an Experiment",
    "section": "Source Code",
    "text": "Source Code\n\n\nSee source code\n\n\n\nCode\nwriteLines(readLines(\"https://raw.githubusercontent.com/ddimmery/softblock/master/r_implementation.R\"))\n\n\nlibrary(Matrix)\nlibrary(igraph)\nlibrary(FNN)\nlibrary(hash)\n\nassign_greedily &lt;- function(graph) {\n    adj_mat = igraph::as_adjacency_matrix(graph, type=\"both\", sparse=TRUE) != 0\n    N = nrow(adj_mat)\n    root_id = make.keys(sample(N, 1))\n\n    a = rbinom(1, 1, 0.5)\n    visited = hash()\n\n    random_order = make.keys(sample(N))\n    unvisited = hash(random_order, random_order)\n\n    colors = hash()\n    stack = hash()\n\n    stack[[root_id]] &lt;- a\n    tentative_color = rbinom(N, 1, 0.5)\n    while ((!is.empty(unvisited)) || (!is.empty(stack))) {\n        if (is.empty(stack)) {\n            cur_node = keys(unvisited)[1]\n            del(cur_node, unvisited)\n            color = tentative_color[as.integer(cur_node)]\n        } else {\n            cur_node = keys(stack)[1]\n            color = stack[[cur_node]]\n            del(cur_node, stack)\n            del(cur_node, unvisited)\n        }\n        visited[[cur_node]] = cur_node\n        colors[[cur_node]] = color\n        children = make.keys(which(adj_mat[as.integer(cur_node), ]))\n        for (child in children) {\n            if(has.key(child, unvisited)) {\n                stack[[child]] = 1 - color\n            }\n        }\n    }\n    values(colors, keys=1:N)\n}\n\n\nassign_softblock &lt;- function(.data, cols, .s2=2, .neighbors=6) {\n    expr &lt;- rlang::enquo(cols)\n    pos &lt;- tidyselect::eval_select(expr, data = .data)\n    df_cov &lt;- rlang::set_names(.data[pos], names(pos))\n    cov_mat = scale(model.matrix(~.+0, df_cov))\n    N = nrow(cov_mat)\n    st = lubridate::now()\n    knn = FNN::get.knn(cov_mat, k=.neighbors)\n    st = lubridate::now()\n    knn.adj = Matrix::sparseMatrix(i=rep(1:N, .neighbors), j=c(knn$nn.index), x=exp(-c(knn$nn.dist) / .s2))\n    knn.graph &lt;- graph_from_adjacency_matrix(knn.adj, mode=\"plus\", weighted=TRUE, diag=FALSE)\n    E(knn.graph)$weight &lt;- (-1 * E(knn.graph)$weight)\n    st = lubridate::now()\n    mst.graph = igraph::mst(knn.graph)\n    E(mst.graph)$weight &lt;- (-1 * E(mst.graph)$weight)\n    st = lubridate::now()\n    assignments &lt;- assign_greedily(mst.graph)\n    .data$treatment &lt;- assignments\n    attr(.data, \"laplacian\") &lt;- igraph::laplacian_matrix(mst.graph, normalize=TRUE, sparse=TRUE)\n    .data\n}\n\nassign_greedy_neighbors &lt;- function(.data, cols) {\n    expr &lt;- rlang::enquo(cols)\n    pos &lt;- tidyselect::eval_select(expr, data = .data)\n    df_cov &lt;- rlang::set_names(.data[pos], names(pos))\n    cov_mat = scale(model.matrix(~.+0, df_cov))\n    N = nrow(cov_mat)\n    knn = FNN::get.knn(cov_mat, k=1)\n    knn.adj = Matrix::sparseMatrix(i=1:N, j=c(knn$nn.index), x=c(knn$nn.dist))\n    knn.graph &lt;- graph_from_adjacency_matrix(knn.adj, mode=\"plus\", weighted=TRUE, diag=FALSE)\n    assignments &lt;- assign_greedily(knn.graph)\n    .data$treatment &lt;- assignments\n    attr(.data, \"laplacian\") &lt;- igraph::laplacian_matrix(knn.graph, normalize=TRUE, sparse=TRUE)\n    .data\n}\n\nassign_matched_pairs &lt;- function(.data, cols, .s2=2, .neighbors=6) {\n    expr &lt;- rlang::enquo(cols)\n    pos &lt;- tidyselect::eval_select(expr, data = .data)\n    df_cov &lt;- rlang::set_names(.data[pos], names(pos))\n    cov_mat = scale(model.matrix(~.+0, df_cov))\n    N = nrow(cov_mat)\n    knn = FNN::get.knn(cov_mat, k=.neighbors)\n    knn.adj = Matrix::sparseMatrix(i=rep(1:N, .neighbors), j=c(knn$nn.index), x=exp(-c(knn$nn.dist) / .s2))\n    knn.graph &lt;- graph_from_adjacency_matrix(knn.adj, mode=\"plus\", weighted=TRUE, diag=FALSE)\n    E(knn.graph)$weight &lt;- (-1 * E(knn.graph)$weight)\n    mwm.graph = igraph::max_bipartite_match(knn.graph)\n    E(mwm.graph)$weight &lt;- (-1 * E(mwm.graph)$weight)\n    assignments &lt;- assign_greedily(mwm.graph)\n    .data$treatment &lt;- assignments\n    attr(.data, \"laplacian\") &lt;- igraph::laplacian_matrix(mwm.graph, normalize=TRUE, sparse=TRUE)\n    .data\n}\n\n# library(tibble)\n# data = tibble(\n#     x1=runif(10),\n#     x2=runif(10),\n#     x3=rbinom(10, 1, 0.5)\n# )\n# library(dplyr)\n# library(tidyr)\n# library(ggplot2)\n# data %&gt;% assign_softblock(c(x1, x2)) -&gt; newdata\n\n# ggplot(newdata, aes(x=x1, y=x2, color=factor(treatment), shape=factor(x3))) + geom_point() + theme_minimal()\n\n# newdata %&gt;%\n#     attr(\"laplacian\") %&gt;%\n#     ifelse(lower.tri(.), ., 0) %&gt;%\n#     as_tibble() -&gt; adj_df\n# names(adj_df) &lt;- paste0(1:ncol(adj_df))\n\n# adj_df %&gt;%\n#     group_by(id_1=as.character(row_number())) %&gt;%\n#     gather(id_2, weight, -id_1) %&gt;%\n#     filter(weight != 0)  %&gt;%\n#     mutate(id_2=as.character(id_2), id=paste(id_1, id_2, sep='-')) -&gt; adj_df\n\n# locs = newdata %&gt;% mutate(id=as.character(row_number())) %&gt;% select(id, x1, x2, x3)\n\n# edges=bind_rows(\n# adj_df  %&gt;% inner_join(locs, by=c('id_1'='id')),\n# adj_df  %&gt;% inner_join(locs, by=c('id_2'='id'))\n# ) %&gt;% arrange(id) %&gt;% ungroup()\n\n# pp = ggplot(newdata, aes(x=x1, y=x2)) +\n# geom_line(aes(group=id, size=1), data=edges, color='grey') +\n# geom_point(aes(color=factor(treatment), shape=factor(x3), size=2), alpha=.9) +\n# scale_size_continuous(range=c(1, 3)) +\n# theme_minimal() + theme(legend.position='none')\n\n# print(pp)"
  },
  {
    "objectID": "posts/softblock-demo/index.html#get-precinct-data",
    "href": "posts/softblock-demo/index.html#get-precinct-data",
    "title": "Using SoftBlock to Design an Experiment",
    "section": "Get Precinct data",
    "text": "Get Precinct data\n\nResults Data\n\n\nCode\nurl &lt;- \"http://dl.ncsbe.gov/ENRS/2020_11_03/results_pct_20201103.zip\"\nzip_file &lt;- tempfile(fileext = \".zip\")\ndownload.file(url, zip_file, mode = \"wb\")\nspec = cols(\n  County = col_character(),\n  `Election Date` = col_character(),\n  Precinct = col_character(),\n  `Contest Group ID` = col_double(),\n  `Contest Type` = col_character(),\n  `Contest Name` = col_character(),\n  Choice = col_character(),\n  `Choice Party` = col_character(),\n  `Vote For` = col_double(),\n  `Election Day` = col_double(),\n  `One Stop` = col_double(),\n  `Absentee by Mail` = col_double(),\n  Provisional = col_double(),\n  `Total Votes` = col_double(),\n  `Real Precinct` = col_character(),\n  X16 = col_skip()\n)\nresults &lt;- readr::read_tsv(zip_file, col_types=spec)\n\n\nNew names:\n• `` -&gt; `...16`\n\n\nWarning: The following named parsers don't match the column names: X16\n\n\n\n\nShapefiles\n\n\nCode\nurl = \"https://s3.amazonaws.com/dl.ncsbe.gov/ShapeFiles/Precinct/SBE_PRECINCTS_20201018.zip\"\ntemp &lt;- tempfile()\ntemp2 &lt;- tempfile()\ndownload.file(url, temp)\nunzip(zipfile = temp, exdir = temp2)\nnc_SHP_file &lt;- list.files(temp2, pattern = \".shp$\",full.names=TRUE)\nshapes &lt;- sf::read_sf(nc_SHP_file)"
  },
  {
    "objectID": "posts/softblock-demo/index.html#aggregate-data-and-join",
    "href": "posts/softblock-demo/index.html#aggregate-data-and-join",
    "title": "Using SoftBlock to Design an Experiment",
    "section": "Aggregate Data and Join",
    "text": "Aggregate Data and Join\n\n\nCode\nresults %&gt;%\n    filter(`Real Precinct` == 'Y') %&gt;%\n    group_by(County, Precinct) %&gt;%\n    summarize(\n        total_vote_pres=sum(`Total Votes`[`Contest Name` == 'US PRESIDENT'], na.rm=TRUE),\n        dem_share_pres=sum(`Total Votes`[`Contest Name` == 'US PRESIDENT' & `Choice Party` == 'DEM'], na.rm=TRUE)/total_vote_pres,\n        gop_share_pres=sum(`Total Votes`[`Contest Name` == 'US PRESIDENT' & `Choice Party` == 'REP'], na.rm=TRUE)/total_vote_pres,\n        total_vote_senate=sum(`Total Votes`[`Contest Name` == 'US SENATE'], na.rm=TRUE),\n        dem_share_senate=sum(`Total Votes`[`Contest Name` == 'US SENATE' & `Choice Party` == 'DEM'], na.rm=TRUE)/total_vote_senate,\n        gop_share_senate=sum(`Total Votes`[`Contest Name` == 'US SENATE' & `Choice Party` == 'REP'], na.rm=TRUE)/total_vote_senate,\n        total_vote_gov=sum(`Total Votes`[`Contest Name` == 'NC GOVERNOR'], na.rm=TRUE),\n        dem_share_gov=sum(`Total Votes`[`Contest Name` == 'NC GOVERNOR' & `Choice Party` == 'DEM'], na.rm=TRUE)/total_vote_gov,\n        gop_share_gov=sum(`Total Votes`[`Contest Name` == 'NC GOVERNOR' & `Choice Party` == 'REP'], na.rm=TRUE)/total_vote_gov,\n        total_vote_house=sum(`Total Votes`[grepl('US HOUSE OF REPRESENTATIVES DISTRICT', `Contest Name`)], na.rm=TRUE),\n        dem_share_house=sum(`Total Votes`[grepl('US HOUSE OF REPRESENTATIVES DISTRICT', `Contest Name`) & `Choice Party` == 'DEM'], na.rm=TRUE)/total_vote_house,\n        gop_share_house=sum(`Total Votes`[grepl('US HOUSE OF REPRESENTATIVES DISTRICT', `Contest Name`) & `Choice Party` == 'REP'], na.rm=TRUE)/total_vote_house\n    ) %&gt;% ungroup() -&gt; results_agg\n\n\n`summarise()` has grouped output by 'County'. You can override using the\n`.groups` argument.\n\n\nCode\ninner_join(results_agg, shapes, by=c('County'='county_nam', 'Precinct'='prec_id')) -&gt; df_joined\nDT::datatable(df_joined %&gt;% sample_n(size=100) %&gt;% dplyr::select(-geometry), rownames = FALSE, options=list(scrollX=TRUE, autoWidth = TRUE))"
  },
  {
    "objectID": "posts/softblock-demo/index.html#add-some-geographic-features",
    "href": "posts/softblock-demo/index.html#add-some-geographic-features",
    "title": "Using SoftBlock to Design an Experiment",
    "section": "Add some geographic features",
    "text": "Add some geographic features\n\n\nCode\ndf_joined$geometry %&gt;%\n    st_centroid() %&gt;%\n    st_transform(\"+init=epsg:4326\") %&gt;%\n    st_coordinates() -&gt; latlong\n\n\nWarning in CPL_crs_from_input(x): GDAL Message 1: +init=epsg:XXXX syntax is\ndeprecated. It might return a CRS with a non-EPSG compliant axis order.\n\n\nCode\ndf_joined$longitude = latlong[, 'X']\ndf_joined$latitude = latlong[, 'Y']\narea = df_joined$geometry %&gt;% st_transform(\"+init=epsg:4326\") %&gt;% st_area()\ndf_joined$area_km2 = units::drop_units(area) / 1e6 # convert m^2 to km^2\ndf_joined &lt;- df_joined %&gt;% mutate(vote_density_pres = total_vote_pres / area_km2)"
  },
  {
    "objectID": "posts/softblock-demo/index.html#simulate-each-design",
    "href": "posts/softblock-demo/index.html#simulate-each-design",
    "title": "Using SoftBlock to Design an Experiment",
    "section": "Simulate each design",
    "text": "Simulate each design\n\nSoftblockGreedy NeighborsQuickBlock\n\n\n\n\nCode\nsimulate_power = create_power_simulator(softblock_weights, df_joined$treatment_sb, df_joined$dem_share_pres)\nestimate_power_for_effect = function(effect) mean(replicate(25, simulate_power(effect=effect)))\neffects = seq(0, 0.05, length=25)\npower_sb = unlist(purrr::map(effects, estimate_power_for_effect))\n\n\n\n\n\n\nCode\nsimulate_power = create_power_simulator(nn_weights, df_joined$treatment_nn, df_joined$dem_share_pres)\nestimate_power_for_effect = function(effect) mean(replicate(25, simulate_power(effect=effect)))\npower_nn = unlist(purrr::map(effects, estimate_power_for_effect))\n\n\n\n\n\n\nCode\nsimulate_power = create_power_simulator_qb(qb_blocks, df_joined$treatment_qb, df_joined$dem_share_pres)\nestimate_power_for_effect = function(effect) mean(replicate(25, simulate_power(effect=effect)))\neffects = seq(0, 0.05, length=25)\npower_qb = unlist(purrr::map(effects, estimate_power_for_effect))"
  },
  {
    "objectID": "posts/softblock-demo/index.html#power-comparison",
    "href": "posts/softblock-demo/index.html#power-comparison",
    "title": "Using SoftBlock to Design an Experiment",
    "section": "Power Comparison",
    "text": "Power Comparison\n\n\nCode\nggplot(\n    tibble(\n        effects=c(effects, effects, effects),\n        power=c(power_sb, power_nn, power_qb),\n        design=c(rep('SoftBlock', length(power_sb)), rep('Greedy Neighbors', length(power_sb)), rep('QuickBlock', length(power_sb)))\n    ), aes(effects, power, color=design)) + geom_line() +\n    scale_x_continuous('Effect (pp)', labels=scales::percent) +\n    scale_y_continuous(\"Power\", labels=scales::percent) +\n    scale_color_discrete(\"Design\") +\n    theme_minimal()"
  },
  {
    "objectID": "posts/softblock-demo/index.html#average-effects",
    "href": "posts/softblock-demo/index.html#average-effects",
    "title": "Using SoftBlock to Design an Experiment",
    "section": "Average Effects",
    "text": "Average Effects\nThe effect estimates here use the appropriate design-based estimators for each design.\nFirst, I’m going to generate a fake outcome to use. I’ll leave the average effect near zero (0.5pp), but individual effects are random draws from around that vale, but with heterogeneous effects based on democratic vote share in 2020 (i.e. positive effects in democratic precincts and vice versa).\n\n\nCode\ndf_joined$outcome = df_joined$dem_share_pres\ndf_joined$ite = with(df_joined, 0.005 + 0.005 * (plogis((dem_share_pres - median(dem_share_pres)) / sd(dem_share_pres))))\ndf_joined$outcome_sb = with(df_joined, outcome + treatment_sb * ite)\ndf_joined$outcome_nn = with(df_joined, outcome + treatment_nn * ite)\ndf_joined$outcome_qb = with(df_joined, outcome + treatment_qb * ite)\n\n\n\nSoftblockGreedy NeighborsQuickBlock\n\n\n\n\nCode\nestimate_effect = function(W, A, Y) {\n    dL = diag(W)\n    Dinv = (2 * A - 1) / dL\n    x_mat = cbind(A, 1)\n    xlx = t(x_mat) %*% W %*% x_mat\n    bread = MASS::ginv(as.matrix(xlx))\n    coefs = bread %*% t(x_mat) %*% W %*% Y\n    r = diag(drop((Y - (x_mat %*% coefs)) ^ 2))\n    meat = t(x_mat) %*% (W %*% r %*% W) %*% x_mat\n    vcv = bread %*% meat %*% bread\n    list(estimate=coefs[1], std.error=sqrt(vcv[1,1]))\n}\nsb_est = estimate_effect(softblock_weights, df_joined$treatment_sb, df_joined$outcome_sb)\n\n\n\n\n\n\nCode\nnn_est = estimate_effect(nn_weights, df_joined$treatment_nn, df_joined$outcome_nn)\n\n\n\n\n\n\nCode\nresult = quickblock::blocking_estimator(df_joined$outcome_qb, qb_blocks, df_joined$treatment_qb)\nqb_est = list(estimate=result$effects[2,1], std.error=sqrt(result$effect_variances[2,1]))"
  },
  {
    "objectID": "posts/softblock-demo/index.html#plot-average-effects",
    "href": "posts/softblock-demo/index.html#plot-average-effects",
    "title": "Using SoftBlock to Design an Experiment",
    "section": "Plot Average Effects",
    "text": "Plot Average Effects\n\n\nCode\ntibble(\n    estimate=c(sb_est$estimate, nn_est$estimate, qb_est$estimate),\n    std.error=c(sb_est$std.error, nn_est$std.error, qb_est$std.error),\n    design=c(\"SoftBlock\", \"Greedy Neighbors\", \"QuickBlock\")\n) %&gt;%\nggplot(aes(x=design, y=estimate, ymin=estimate-1.96*std.error, ymax=estimate+1.96*std.error)) +\n    geom_pointrange() +\n    scale_x_discrete(\"Design\") +\n    scale_y_continuous(\"ATE (pp)\", labels=scales::percent) +\n    coord_flip() +\n    theme_minimal()"
  },
  {
    "objectID": "posts/softblock-demo/index.html#heterogeneous-effects",
    "href": "posts/softblock-demo/index.html#heterogeneous-effects",
    "title": "Using SoftBlock to Design an Experiment",
    "section": "Heterogeneous Effects",
    "text": "Heterogeneous Effects\nThese effects will be estimated using DR-learner of Kennedy (2020). For simplicity, I will estimate nuisance functions using glmnet.\n\n\nCode\npredict.hte.split = function(x, a, y, s, predict.s=4) {\n    s.pi = (predict.s) %% 4 + 1\n    s.mu = (predict.s + 1) %% 4 + 1\n    s.dr = (predict.s + 2) %% 4 + 1\n    pihat &lt;- predict(cv.glmnet(x[s==s.pi,],a[s==s.pi], family=\"binomial\", nfolds=10), newx=x, type=\"response\", s=\"lambda.min\")\n    mu0hat &lt;- predict(cv.glmnet(x[a==0 & s==s.mu,],y[a==0 & s==s.mu], nfolds=10), newx=x, type=\"response\", s=\"lambda.min\")\n    mu1hat &lt;- predict(cv.glmnet(x[a==1 & s==s.mu,],y[a==1 & s==s.mu], nfolds=10),newx=x, type=\"response\", s=\"lambda.min\")\n    pseudo &lt;- ((a-pihat)/(pihat*(1-pihat)))*(y-a*mu1hat-(1-a)*mu0hat) + mu1hat - mu0hat\n    drl &lt;- predict(cv.glmnet(x[s==s.dr,],pseudo[s==s.dr]),newx=x[s==predict.s, ], s=\"lambda.min\")\n    drl\n}\npredict.hte.crossfit = function(x, a, y) {\n    N = length(a)\n    s = sample(1:4, N, replace=TRUE)\n    hte = rep(NA_real_, N)\n    for (split in 1:4) {\n        hte[s==split] = predict.hte.split(x, a, y, s, predict.s=split)\n    }\n    hte\n}\ncalculate_hte &lt;- function(.data, cols, .treatment='treatment', .outcome='outcome') {\n    expr &lt;- rlang::enquo(cols)\n    pos &lt;- tidyselect::eval_select(expr, data = .data)\n    df_cov &lt;- rlang::set_names(.data[pos], names(pos))\n    cov_mat = scale(model.matrix(~.+0, df_cov))\n    .data$hte = predict.hte.crossfit(cov_mat, .data[[.treatment]], .data[[.outcome]])\n    .data\n}"
  },
  {
    "objectID": "posts/softblock-demo/index.html#estimate-htes",
    "href": "posts/softblock-demo/index.html#estimate-htes",
    "title": "Using SoftBlock to Design an Experiment",
    "section": "Estimate HTEs",
    "text": "Estimate HTEs\n\nSoftblockGreedy NeighborsQuickBlock\n\n\n\n\nCode\ndf_joined %&gt;% calculate_hte(c(\n    longitude, latitude, area_km2, vote_density_pres, # geographic\n    total_vote_pres, # 2020 presidential\n    total_vote_senate, dem_share_senate, gop_share_senate, # 2020 senate\n    total_vote_gov, dem_share_gov, gop_share_gov, # 2020 governor\n    total_vote_house, dem_share_house, gop_share_house # 2020 house\n), .treatment='treatment_sb', .outcome='outcome_sb') %&gt;% rename(hte_sb=hte) -&gt; df_joined\nsummary(df_joined$hte_sb)\n\n\n     Min.   1st Qu.    Median      Mean   3rd Qu.      Max. \n-0.010306  0.004135  0.007536  0.007148  0.009029  0.033580 \n\n\n\n\n\n\nCode\ndf_joined %&gt;% calculate_hte(c(\n    longitude, latitude, area_km2, vote_density_pres, # geographic\n    total_vote_pres, # 2020 presidential\n    total_vote_senate, dem_share_senate, gop_share_senate, # 2020 senate\n    total_vote_gov, dem_share_gov, gop_share_gov, # 2020 governor\n    total_vote_house, dem_share_house, gop_share_house # 2020 house\n), .treatment='treatment_nn', .outcome='outcome_nn') %&gt;% rename(hte_nn=hte) -&gt; df_joined\nsummary(df_joined$hte_nn)\n\n\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n0.005742 0.006825 0.007388 0.007670 0.008596 0.010863 \n\n\n\n\n\n\nCode\ndf_joined %&gt;% calculate_hte(c(\n    longitude, latitude, area_km2, vote_density_pres, # geographic\n    total_vote_pres, # 2020 presidential\n    total_vote_senate, dem_share_senate, gop_share_senate, # 2020 senate\n    total_vote_gov, dem_share_gov, gop_share_gov, # 2020 governor\n    total_vote_house, dem_share_house, gop_share_house # 2020 house\n), .treatment='treatment_qb', .outcome='outcome_qb') %&gt;% rename(hte_qb=hte) -&gt; df_joined\nsummary(df_joined$hte_qb)\n\n\n      Min.    1st Qu.     Median       Mean    3rd Qu.       Max. \n-0.0007862  0.0070832  0.0083612  0.0082249  0.0094074  0.0398552"
  },
  {
    "objectID": "posts/softblock-demo/index.html#plot-hte-distributions",
    "href": "posts/softblock-demo/index.html#plot-hte-distributions",
    "title": "Using SoftBlock to Design an Experiment",
    "section": "Plot HTE distributions",
    "text": "Plot HTE distributions\n\n\nCode\nggplot(df_joined, aes()) +\ngeom_histogram(aes(x=hte_sb, fill='SoftBlock'), bins=50, alpha=0.4) +\ngeom_histogram(aes(x=hte_nn, fill='Greedy Neighbors'), bins=50, alpha=0.4) +\ngeom_histogram(aes(x=hte_qb, fill='QuickBlock'), bins=50, alpha=0.4) +\nscale_x_continuous(\"Effect (pp)\", labels=scales::percent) +\ntheme_minimal()"
  },
  {
    "objectID": "research.html",
    "href": "research.html",
    "title": "Research",
    "section": "",
    "text": "Published\n\n2024\nYan Leng and Drew Dimmery. (2024) \"Calibration of Heterogeneous Treatment Effects in Randomized Experiments.\"\n        \n        Preprint\n     \n        \n        Published\n    \nHunt Allcott, Matthew Gentzkow, Winter Mason, Arjun Wilkins, Pablo Barberá, Taylor Brown, Juan Carlos Cisneros, Adriana Crespo-Tenorio, Drew Dimmery, Deen Freelon, Sandra González-Bailón, Andrew M. Guess, Young Mie Kim, David Lazer, Neil Malhotra, Devra Moehler, Sameer Nair-Desai, Houda Nait El Barj, Brendan Nyhan, Ana Carolina Paixao de Queiroz, Jennifer Pan, Jaime Settle, Emily Thorson, Rebekah Tromble, Carlos Velasco Rivera, Benjamin Wittenbrink, Magdalena Wojcieszak, Saam Zahedian, Annie Franco, Chad Kiewiet de Jonge, Natalie Jomini Stroud, and Joshua A. Tucker. (2024) \"The effects of Facebook and Instagram on the 2020 election: A deactivation experiment.\" Proceedings of the National Academy of Sciences\n        \n        Published\n    \n2023\nAndrew Guess, Neil Malhotra, Jennifer Pan, Pablo Barberá, Hunt Alcott, Taylor Brown, Adriana Crespo-Tenorio, Drew Dimmery, Deen Freelon, Matthew Gentzkow, Sandra González-Bailón, Edward Kennedy, Young Mie Kim, David Lazer, Devra Moehler, Brendan Nyhan, Carlos Velasco Rivera, Jaime Settle, Daniel Thomas, Emily Thorson, Rebekah Tromble, Arjun Wilkins, Magdalena Wojcieszak, Beixian Xiong, Chad Kiewet de Jong, Annie Franco, Winter Mason, Natalie Jomini Stroud, and Joshua Tucker. (2023) \"How do social media feed algorithms affect attitudes and behavior in an election campaign?.\" Science\n        \n        Published\n    \nAndrew Guess, Neil Malhotra, Jennifer Pan, Pablo Barberá, Hunt Alcott, Taylor Brown, Adriana Crespo-Tenorio, Drew Dimmery, Deen Freelon, Matthew Gentzkow, Sandra González-Bailón, Edward Kennedy, Young Mie Kim, David Lazer, Devra Moehler, Brendan Nyhan, Carlos Velasco Rivera, Jaime Settle, Daniel Thomas, Emily Thorson, Rebekah Tromble, Arjun Wilkins, Magdalena Wojcieszak, Beixian Xiong, Chad Kiewet de Jong, Annie Franco, Winter Mason, Natalie Jomini Stroud, and Joshua Tucker. (2023) \"Reshares on social media amplify political news but do not detectably affect beliefs or opinions.\" Science\n        \n        Published\n    \nBrendan Nyhan, Jaime Settle, Emily Thorson, Magdalena Wojcieszak, Pablo Barberá, Annie Chen, Hunt Alcott, Taylor Brown, Adriana Crespo-Tenorio, Drew Dimmery, Deen Freelon, Matthew Gentzkow, Sandra González-Bailón, Andrew Guess, Edward Kennedy, Young Mie Kim, David Lazer, Neil Malhotra, Devra Moehler, Jennifer Pan, Daniel Thomas, Rebekah Tromble, Carlos Velasco Rivera, Arjun Wilkins, Beixian Xiong, Chad Kiewet de Jong, Annie Franco, Winter Mason, Natalie Jomini Stroud, and Joshua Tucker. (2023) \"Like-minded sources on Facebook are prevalent but not polarizing.\" Nature\n        \n        Published\n    \nMiloš Fišar, Ben Greiner, Christoph Huber, Elena Katok, Ali Ozkes, and Management Science Reproducibility Collaboration. (2023) \"Reproducibility in Management Science.\"\n        \n        Preprint\n     \n        \n        Published\n    \n2022\nDavid Arbour, Drew Dimmery, Tung Mai, and Anup Rao. (2022) \"Online Balanced Experimental Design.\" ICML\n        \n        Preprint\n     \n        \n        Published\n    \nHan Wu, Sarah Tan, Weiwei Li, Mia Garrard, Adam Obeng, Drew Dimmery, Shaun Singh, Hanson Wang, Daniel Jiang, and Eytan Bakshy. (2022) \"Distilling Heterogeneity: From Explanations of Heterogeneous Treatment Effect Models to Interpretable Policies.\"\n        \n        Preprint\n     \n        \n        Published\n    \n2021\nDavid Arbour, Drew Dimmery, and Arjun Sondhi. (2021) \"Permutation Weighting.\" ICML\n        \n        Preprint\n     \n        \n        Published\n    \nDavid Arbour, Drew Dimmery, and Anup Rao. (2021) \"Efficient Balanced Treatment Assignments for Experimentation.\" AISTATS\n        \n        Preprint\n     \n        \n        Github\n     \n        \n        Published\n    \nMy Phan, David Arbour, Drew Dimmery, and Anup Rao. (2021) \"Designing Transportable Experiments Under S-admissability.\" AISTATS\n        \n        Preprint\n     \n        \n        Github\n     \n        \n        Published\n    \n2020\nArjun Sondhi, David Arbour, and Drew Dimmery. (2020) \"Balanced off-policy evaluation in general action spaces.\" AISTATS\n        \n        Preprint\n     \n        \n        Published\n    \n2019\nDrew Dimmery, Eytan Bakshy, and Jasjeet Sekhon. (2019) \"Shrinkage Estimators in Online Experiments.\" KDD\n        \n        Preprint\n     \n        \n        Published\n    \n2016\nDrew Dimmery and Andrew Peterson. (2016) \"Shining the Light on Dark Money: Political Spending by Nonprofits.\" RSF: The Russell Sage Foundation Journal of the Social Sciences\n        \n        Published\n    \n2012\nC.T Kullenberg, S.R. Mishra, Drew Dimmery, and the NOMAD Collaboration. (2012) \"A search for single photon events in neutrino interactions.\" Physics Letters B\n        \n        Published\n    \n\nWorking Papers / Non-archival\n\n2021\nMolly Offer-Westort and Drew Dimmery. (2021) \"Experimentation for Homogenous Policy Change.\"\n        \n        Preprint\n    \n2019\nHongzi Mao, Shannon Chen, Drew Dimmery, Shaun Singh, Drew Blaisdell, Yuandong Tian, Mohammad Alizadeh, and Eytan Bakshy. (2019) \"Real-world Video Adaptation with Reinforcement Learning.\" ICML Workshop - RL4RealLife\n        \n        Preprint\n    \nSam Daulton, Shaun Singh, Vashist Avadhanula, Drew Dimmery, and Eytan Bakshy. (2019) \"Thompson Sampling for Contextual Bandit Problems with Auxiliary Safety Constraints.\" NeurIPS Workshop - Safety and Robustness in Decision Making\n        \n        Preprint"
  },
  {
    "objectID": "posts/quarto-website/index.html",
    "href": "posts/quarto-website/index.html",
    "title": "Quarto for an Academic Website",
    "section": "",
    "text": "I’ve never been good at keeping my website updated. I always go through two different phases of maintenance:\n\nRushing around creating a new website with bells and whistles using whatever the flavor of the month is\nNever updating an existing website\n\nI’m hoping to break out of this cycle, but am currently solidly within Phase 1.\n\nA highlight from my time in Phase 2 was when I forgot to update my DNS and I totally lost control of drewdimmery.com (don’t go there, it has a squatter). I think my website at that time was some Octopress monstrosity. There are a few reasons I think Quarto might help with my vicious circle.\n\nServing static HTML pages is about as easy as it gets\nVery little Quarto-specific syntax to recall (e.g. CLI commands or abstruse markup)\nLots of flexibility (Python / R) in how to generate that static content\nFull programmability means that generation can be based on arbitrary data structures of my choosing\n\nI previously used Hugo Academic for building my website, which was much better than just editing the content directly, but I never remembered the right way to generate a new publication definition (there was a CLI, but I never remembered the syntax). Each publication got its own file describing its details, and I found this quite clunky. I wanted something extremely lightweight: there isn’t much reason for my individual publications to get pages of their own, and I really don’t need a lot of information on each of them. I just want some basic information about each and a set of appropriate links to more details.\nThis post will detail how I’ve set up Quarto to accomplish this task. I’ve nearly completely separated the two main concerns around maintaining an academic website / CV, which to me are data on publications and software from the design elements of how to display them. It’s entirely possible that my particular issues are unique and this post won’t be useful to anyone else. Luckily, the marginal cost of words on the internet is essentially zero (and maybe the marginal value is, too)."
  },
  {
    "objectID": "posts/quarto-website/index.html#data",
    "href": "posts/quarto-website/index.html#data",
    "title": "Quarto for an Academic Website",
    "section": "Data",
    "text": "Data\nI put data about each publication in a basic YAML format:\n\n\nSee example data\n\nsoftblock:\n  title: Efficient Balanced Treatment Assignments for Experimentation\n  authors:\n    - David Arbour\n    - me\n    - Anup Rao\n  year: 2021\n  venue: AISTATS\n  preprint: https://arxiv.org/abs/2010.11332\n  published_url: https://proceedings.mlr.press/v130/arbour21a.html\n  github: https://github.com/ddimmery/softblock\n\nThis is basically like a simplified bibtex entry with more URLs so I can annotate where to find replication materials for a given paper, as well as distinguish between preprints (always freely accessible) versus published versions (not always open access). A convenience that I add in the markup here is referring to myself as me in the author list (which is an ordered list). This allows me to add in extra post-processing to highlight where I sit in the author list.\nSome additional things I considered adding but chose to ignore for a first version:\n\nAn abstract\nA suggested bibtex entry\n\nBoth of these would be easy to add, but I chose to start simpler. I don’t love YAML for entering long blocks of text, which both of these are."
  },
  {
    "objectID": "posts/quarto-website/index.html#formatting",
    "href": "posts/quarto-website/index.html#formatting",
    "title": "Quarto for an Academic Website",
    "section": "Formatting",
    "text": "Formatting\nSince I can write the generation logic for page in Python, this puts me on comfortable ground to hack something together. To knit the above publication data into HTML, I just literally bind together the programmatically generated raw HTML and print it onto the page.\nI do a couple additional useful things in this process: - Separate out working papers or non-archival papers from published work (I make this distinction based on whether I include a published_url field or not). - Order and categorize papers by year - Provide nice Bootstrappy buttons for external links (e.g. to Preprints / Code / etc)\n\n\nSee research.qmd fragment\n\nimport yaml\nfrom IPython.display import display, Markdown, HTML\n\ndef readable_list(_s):\n  if len(_s) &lt; 3:\n    return ' and '.join(map(str, _s))\n  *a, b = _s\n  return f\"{', '.join(map(str, a))}, and {b}\"\n\ndef button(url, str, icon):\n    icon_base = icon[:2]\n    return f\"\"\"&lt;a class=\"btn btn-outline-dark btn-sm\", href=\"{url}\" target=\"_blank\" rel=\"noopener noreferrer\"&gt;\n        &lt;i class=\"{icon_base} {icon}\" role='img' aria-label='{str}'&gt;&lt;/i&gt;\n        {str}\n    &lt;/a&gt;\"\"\"\n\nyaml_data = yaml.safe_load(open(\"papers.yaml\"))\npub_strs = {\"pubs\": {}, \"wps\": {}}\nfor _, data in yaml_data.items():\n    title_str = data[\"title\"]\n    authors = data.get(\"authors\", [\"me\"])\n    authors = [\n        aut if aut != \"me\" else \"&lt;strong&gt;Drew Dimmery&lt;/strong&gt;\" for aut in authors\n    ]\n    author_str = readable_list(authors)\n    year_str = data[\"year\"]\n\n    buttons = []\n    preprint = data.get(\"preprint\")\n    if preprint is not None:\n        buttons.append(button(preprint, \"Preprint\", \"bi-file-earmark-pdf\"))\n\n    github = data.get(\"github\")\n    if github is not None:\n        buttons.append(button(github, \"Github\", \"bi-github\"))\n\n    pub_url = data.get(\"published_url\")\n    venue = data.get(\"venue\")\n    working_paper = pub_url is None\n    \n    pub_str = f'{author_str}. ({year_str}) \"{title_str}.\"'\n\n    if venue is not None:\n        pub_str += f\" &lt;em&gt;{venue}&lt;/em&gt;\"\n\n    if working_paper:\n        if year_str not in pub_strs[\"wps\"]:\n            pub_strs[\"wps\"][year_str] = []\n        pub_strs[\"wps\"][year_str].append(\n            \"&lt;li class='list-group-item'&gt;\" + pub_str + \"&lt;br&gt;\" + \" \".join(buttons) + \"&lt;/li&gt;\"\n        )\n    else:\n        if year_str not in pub_strs[\"pubs\"]:\n            pub_strs[\"pubs\"][year_str] = []\n        buttons.append(button(pub_url, \"Published\", \"ai-archive\"))\n        pub_strs[\"pubs\"][year_str].append(\n            \"&lt;li class='list-group-item'&gt;\" + pub_str + \"&lt;br&gt;\" + \" \".join(buttons) + \"&lt;/li&gt;\"\n        )\n\nI then print this out using the display functions from the IPython module and using the asis chunk option:\n\n\nSee research.qmd fragment\n\nfor year in sorted(pub_strs[\"pubs\"].keys(), reverse=True):\n    display(Markdown(f\"### {year}\" + \"{#\" + f\"published-{year}\" + \"}\"))\n    display(HTML(\n        \"&lt;ul class='list-group list-group-flush'&gt;\" + '\\n'.join(pub_strs[\"pubs\"][year]) + \"&lt;/ul&gt;\"\n    ))\n\nThe full code is on GitHub.\nIt’s worth noting that to get the years to show up in the Table of Contents its necessary to be careful exactly how the content is stuck onto the page. If you don’t use the asis chunk option, you can still get all the right content to show up, but it won’t necessarily appear in the ToC. I also found it necessary to include section-divs: false in the header, or else the output would get wrapped in additional div tags which made it harder to get the right classes in the right divs. There are probably more elegant ways to do all of this.\nI use the same basic setup to populate the Software page, albeit with simpler logic.\n\nAdditions\nI debated adding an abstract that expands out on click (like the code folding above in this post). This would actually be more or less trivial to add using a &lt;details&gt; HTML tag if I wanted to provide the data in the YAML. I’m ignoring this for now because I want to minimize data entry for my future self (and it’s anyway just a click away at the Preprint link)."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Drew Dimmery",
    "section": "",
    "text": "I am the Professor of Data Science for the Common Good at the Hertie School’s Data Science Lab.\nI previously worked on the Adaptive Experimentation team at Facebook Core Data Science in New York, and at the Research Network Data Science at the University of Vienna.\nMy research focuses on the intersection of machine learning and causal inference. I work a lot on experimental design and I think hard about how experimentation can be better and more efficient. I’m also interested in better applying the machinery of machine learning to observational causal inference.\nI’m most interested in methodological research useful for the study of social phenomena."
  },
  {
    "objectID": "software.html",
    "href": "software.html",
    "title": "Software",
    "section": "",
    "text": "tidyhte\ntidyhte provides tidy semantics for estimation of heterogeneous treatment effects through the use of Kennedy’s (n.d.) doubly-robust learner.\nThe goal of tidyhte is to use a sort of “recipe” design. This should (hopefully) make it extremely easy to scale an analysis of HTE from the common single-outcome / single-moderator case to many outcomes and many moderators. The configuration of tidyhte should make it extremely easy to perform the same analysis across many outcomes and for a wide-array of moderators. It’s written to be fairly easy to extend to different models and to add additional diagnostics and ways to output information from a set of HTE estimates.\n\n        \n        Website\n     \n        \n        Github\n    \nregweight\nThe goal of regweight is to make it easy to diagnose a model using Aronow and Samii (2015) regression weights.\nIn short, these weights show which observations are most influential for determining the observed value of a coefficient in a linear regression. If the linear regression is aiming to estimate causal effects, this implies that the OLS estimand may differ from the average treatment effect. These linear regression weights provide, in some sense, the most precise estimate available given a conditioning set (and a linear model). These weights are in expectation the conditional variance of the variable of interest (given the other covariates in the model).\n\n        \n        Website\n     \n        \n        Github\n     \n        \n        Package\n    \nrdd\nOutdated! Users should switch to actively maintained and updated RD tools.\nProvides the tools to undertake estimation in Regression Discontinuity Designs. Both sharp and fuzzy designs are supported. Estimation is accomplished using local linear regression. A provided function will utilize Imbens-Kalyanaraman optimal bandwidth calculation. A function is also included to test the assumption of no-sorting effects.\n\n        \n        Github\n     \n        \n        Package"
  }
]