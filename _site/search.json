[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "Subscription\n\n\n\n\n\nSubscribe to the RSS feed for updates.\nIf you’d prefer to receive the blog as a newsletter, Feedrabbit is a good and simple choice (until I make something better).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat’s the point of RCTs?\n\n\n\n\n\n\nmethodology\n\n\nexperiments\n\n\n\nAn ontological perspective on the value of experimentation\n\n\n\n\n\nFeb 13, 2025\n\n\nDrew Dimmery\n\n\n\n\n\n\n\n\n\n\n\n\nWhat was US2020?\n\n\n\n\n\n\nsocial-media\n\n\nmetascience\n\n\n\nThe dialectics of big internet research\n\n\n\n\n\nJan 29, 2025\n\n\nDrew Dimmery\n\n\n\n\n\n\n\n\n\n\n\n\nIs Bluesky convivial?\n\n\n\n\n\n\ntechnology\n\n\nsocial-media\n\n\n\nSmalltalk and the impossibility of social toolmaking to empower end-users\n\n\n\n\n\nNov 21, 2024\n\n\nDrew Dimmery\n\n\n\n\n\n\n\n\n\n\n\n\nExamining the Apparatus\n\n\n\n\n\n\nsocial-media\n\n\ntechnology\n\n\n\nThe purpose of a system is what it does!\n\n\n\n\n\nJan 30, 2024\n\n\nDrew Dimmery\n\n\n\n\n\n\n\n\n\n\n\n\nCalibration as an HTE diagnostic\n\n\n\n\n\n\nmethodology\n\n\nexperiments\n\n\n\nPaper just accepted to Information Systems Research\n\n\n\n\n\nJan 16, 2024\n\n\nDrew Dimmery\n\n\n\n\n\n\n\n\n\n\n\n\nPlagiarism is bad\n\n\n\n\n\n\nmetascience\n\n\n\nPapers are what we do: we should try to do them well\n\n\n\n\n\nJan 4, 2024\n\n\nDrew Dimmery\n\n\n\n\n\n\n\n\n\n\n\n\nStop looking for the next Twitter\n\n\n\n\n\n\nsocial-media\n\n\ntechnology\n\n\n\nBlueSky doesn’t solve any of Twitter’s real problems, nor will any other microblog\n\n\n\n\n\nOct 3, 2023\n\n\nDrew Dimmery\n\n\n\n\n\n\n\n\n\n\n\n\nA Blueprint for the Regulation of Tech\n\n\n\n\n\n\ntechnology\n\n\nmetascience\n\n\n\nCounterfactuals are the key to accurately assessing the risks of online platforms\n\n\n\n\n\nSep 11, 2023\n\n\nDrew Dimmery\n\n\n\n\n\n\n\n\n\n\n\n\nQuarto for an Academic Website\n\n\n\n\n\n\nwebsite\n\n\n\nI continue my long search for a way to generate a nicely formatted website with publication list based on adding publication information to a single source of truth without re-remembering how all the formatting works each time. \n\n\n\n\n\nMay 11, 2022\n\n\nDrew Dimmery\n\n\n\n\n\n\n\n\n\n\n\n\nUsing SoftBlock to Design an Experiment\n\n\n\n\n\n\nexperiments\n\n\ndemo\n\n\n\nIn this demo, I’m going to walk through an example experimental design using methods from our recent paper on Balanced Design.\n\n\n\n\n\nMay 10, 2022\n\n\nDrew Dimmery\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I am the Professor of Data Science for the Common Good at the Hertie School’s Data Science Lab.\n\nResearch Interests\n\nCausal Inference\nMachine Learning\nData Science\nExperimental Design\n\n\n\nEducation\n\n\nNew York University\nPhD in Political Methodology\nAdvisors: Cyrus Samii, Neal Beck, Josh Tucker\n\n\nNew York, NY\nGranted May 2016\n\n\n\nDissertation: Essays on Causal Inference and Machine Learning with Application to Nonprofits\nWinner of 2015 Williams Award for Best Dissertation Proposal in Political Methodology from the Society of Political Methodology\n\n\n\nUNC Chapel Hill\nB.A. in International and Area Studies with distinction\n\n\nChapel Hill, NC\nGranted June 2010\n\n\n\n\nExperience\n\n\nHertie School\n\n\nJanuary 2024 - present\n\n\nProfessor of Data Science for the Common Good\n\n\nUniversity of Vienna\n\n\nApril 2021 - December 2023\n\n\nScientific Coordinator\n\nSupervisor: Philipp Grohs\n\n\n\nFacebook Core Data Science\n\n\nSept 2016 - March 2021\n\n\nResearch Scientist\n\nPart of Eytan Bakshy’s Adaptive Experimentation team\nDeveloped statistical, machine learning and experimental methodology\nRan adaptive and contextual field experiments with a variety of product teams\nIntegrated advanced methodologies into a toolkit for scalable and automatic experimentation intended for optimization (Ax) - released at F8 2019)\nDeveloped scalable methods for robust observational causal inference as the technical lead of our “CausalML” initiative\n\n\n\nPrinceton University\n\n\nSeptember 2015 - May 2016\n\n\nPre-doctoral fellow\n\nSupervised by Kosuke Imai\n\n\n\nFacebook Core Data Science\n\n\nSummer 2015\n\n\nSummer Intern\n\nStatistical and Decision Science Team\nSupervised by Eytan Bakshy\n\n\n\nTeaching\n\nHertie School\n\n\nCausal Machine Learning\n\n\n2024-\n\n\n\n\nData Structures & Algorithms\n\n\n2024-\n\n\n\n\nMachine Learning\n\n\n2024-\n\n\n\n\nFacebook\n\n\nInternal datacamp class on designing and analyzing experiments\n\n\n2017-2019\n\n\n\n\nNYU Undergraduate\n\n\nTA for Power and Politics in America (under Jonathan Nagler)\n\n\nFall 2014\n\n\n\n\nTA for Games, Strategy and Politics (under Steven Brams)\n\n\nFall 2013\n\n\n\n\nNYU Graduate\n\n\nTA for Quantitative Methods II (under Nathaniel Beck)\n\n\nSpring 2015\n\n\n\n\nTA for Quantitative Methods II (under Cyrus Samii)\n\n\nSpring 2014\n\n\n\n\nHigh Performance Computing Talk for NYU Datalab\n\n\nFebruary 2014\n\n\n\n\nIntroduction to R for NYU Datalab\n\n\nJanuary 2013"
  },
  {
    "objectID": "posts/softblock-demo/index.html",
    "href": "posts/softblock-demo/index.html",
    "title": "Using SoftBlock to Design an Experiment",
    "section": "",
    "text": "In particular, I’m going to imagine that I’m designing an experiment in which I assign different treatments to particular precincts in North Carolina. In order to optimize power, of course, we want to make sure that our two test groups look as similar as possible in terms of prior voting patterns.\nThus, the steps in this design will be:\n\nCollect relevant historical data.\nDefine variables on which we wish to balance.\nAllocate treatment assignment using new methods.\nSimulate the power of hypothesis tests under the proposed design.\nFake some outcome data and analyze it for average and heterogeneous treatment effects."
  },
  {
    "objectID": "posts/softblock-demo/index.html#description",
    "href": "posts/softblock-demo/index.html#description",
    "title": "Using SoftBlock to Design an Experiment",
    "section": "Description",
    "text": "Description\nThe relevant API is a function with tidyverse semantics called assign_softblock (or assign_greedy_neighbors). These functions accept a vector of columns to be used in the design. The SoftBlock version additionally accepts two arguments, .s2 for the bandwidth of the RBF kernel to use in the construction of a similarity matrix as well as .neighbors which indicates the number of nearest neighbors to include in the graph on which to construct the spanning tree. These parameters don’t generally need to be modified."
  },
  {
    "objectID": "posts/softblock-demo/index.html#source-code",
    "href": "posts/softblock-demo/index.html#source-code",
    "title": "Using SoftBlock to Design an Experiment",
    "section": "Source Code",
    "text": "Source Code\n\n\nSee source code\n\n\n\nCode\nwriteLines(readLines(\"https://raw.githubusercontent.com/ddimmery/softblock/master/r_implementation.R\"))\n\n\nlibrary(Matrix)\nlibrary(igraph)\nlibrary(FNN)\nlibrary(hash)\n\nassign_greedily &lt;- function(graph) {\n    adj_mat = igraph::as_adjacency_matrix(graph, type=\"both\", sparse=TRUE) != 0\n    N = nrow(adj_mat)\n    root_id = make.keys(sample(N, 1))\n\n    a = rbinom(1, 1, 0.5)\n    visited = hash()\n\n    random_order = make.keys(sample(N))\n    unvisited = hash(random_order, random_order)\n\n    colors = hash()\n    stack = hash()\n\n    stack[[root_id]] &lt;- a\n    tentative_color = rbinom(N, 1, 0.5)\n    while ((!is.empty(unvisited)) || (!is.empty(stack))) {\n        if (is.empty(stack)) {\n            cur_node = keys(unvisited)[1]\n            del(cur_node, unvisited)\n            color = tentative_color[as.integer(cur_node)]\n        } else {\n            cur_node = keys(stack)[1]\n            color = stack[[cur_node]]\n            del(cur_node, stack)\n            del(cur_node, unvisited)\n        }\n        visited[[cur_node]] = cur_node\n        colors[[cur_node]] = color\n        children = make.keys(which(adj_mat[as.integer(cur_node), ]))\n        for (child in children) {\n            if(has.key(child, unvisited)) {\n                stack[[child]] = 1 - color\n            }\n        }\n    }\n    values(colors, keys=1:N)\n}\n\n\nassign_softblock &lt;- function(.data, cols, .s2=2, .neighbors=6) {\n    expr &lt;- rlang::enquo(cols)\n    pos &lt;- tidyselect::eval_select(expr, data = .data)\n    df_cov &lt;- rlang::set_names(.data[pos], names(pos))\n    cov_mat = scale(model.matrix(~.+0, df_cov))\n    N = nrow(cov_mat)\n    st = lubridate::now()\n    knn = FNN::get.knn(cov_mat, k=.neighbors)\n    st = lubridate::now()\n    knn.adj = Matrix::sparseMatrix(i=rep(1:N, .neighbors), j=c(knn$nn.index), x=exp(-c(knn$nn.dist) / .s2))\n    knn.graph &lt;- graph_from_adjacency_matrix(knn.adj, mode=\"plus\", weighted=TRUE, diag=FALSE)\n    E(knn.graph)$weight &lt;- (-1 * E(knn.graph)$weight)\n    st = lubridate::now()\n    mst.graph = igraph::mst(knn.graph)\n    E(mst.graph)$weight &lt;- (-1 * E(mst.graph)$weight)\n    st = lubridate::now()\n    assignments &lt;- assign_greedily(mst.graph)\n    .data$treatment &lt;- assignments\n    attr(.data, \"laplacian\") &lt;- igraph::laplacian_matrix(mst.graph, normalize=TRUE, sparse=TRUE)\n    .data\n}\n\nassign_greedy_neighbors &lt;- function(.data, cols) {\n    expr &lt;- rlang::enquo(cols)\n    pos &lt;- tidyselect::eval_select(expr, data = .data)\n    df_cov &lt;- rlang::set_names(.data[pos], names(pos))\n    cov_mat = scale(model.matrix(~.+0, df_cov))\n    N = nrow(cov_mat)\n    knn = FNN::get.knn(cov_mat, k=1)\n    knn.adj = Matrix::sparseMatrix(i=1:N, j=c(knn$nn.index), x=c(knn$nn.dist))\n    knn.graph &lt;- graph_from_adjacency_matrix(knn.adj, mode=\"plus\", weighted=TRUE, diag=FALSE)\n    assignments &lt;- assign_greedily(knn.graph)\n    .data$treatment &lt;- assignments\n    attr(.data, \"laplacian\") &lt;- igraph::laplacian_matrix(knn.graph, normalize=TRUE, sparse=TRUE)\n    .data\n}\n\nassign_matched_pairs &lt;- function(.data, cols, .s2=2, .neighbors=6) {\n    expr &lt;- rlang::enquo(cols)\n    pos &lt;- tidyselect::eval_select(expr, data = .data)\n    df_cov &lt;- rlang::set_names(.data[pos], names(pos))\n    cov_mat = scale(model.matrix(~.+0, df_cov))\n    N = nrow(cov_mat)\n    knn = FNN::get.knn(cov_mat, k=.neighbors)\n    knn.adj = Matrix::sparseMatrix(i=rep(1:N, .neighbors), j=c(knn$nn.index), x=exp(-c(knn$nn.dist) / .s2))\n    knn.graph &lt;- graph_from_adjacency_matrix(knn.adj, mode=\"plus\", weighted=TRUE, diag=FALSE)\n    E(knn.graph)$weight &lt;- (-1 * E(knn.graph)$weight)\n    mwm.graph = igraph::max_bipartite_match(knn.graph)\n    E(mwm.graph)$weight &lt;- (-1 * E(mwm.graph)$weight)\n    assignments &lt;- assign_greedily(mwm.graph)\n    .data$treatment &lt;- assignments\n    attr(.data, \"laplacian\") &lt;- igraph::laplacian_matrix(mwm.graph, normalize=TRUE, sparse=TRUE)\n    .data\n}\n\n# library(tibble)\n# data = tibble(\n#     x1=runif(10),\n#     x2=runif(10),\n#     x3=rbinom(10, 1, 0.5)\n# )\n# library(dplyr)\n# library(tidyr)\n# library(ggplot2)\n# data %&gt;% assign_softblock(c(x1, x2)) -&gt; newdata\n\n# ggplot(newdata, aes(x=x1, y=x2, color=factor(treatment), shape=factor(x3))) + geom_point() + theme_minimal()\n\n# newdata %&gt;%\n#     attr(\"laplacian\") %&gt;%\n#     ifelse(lower.tri(.), ., 0) %&gt;%\n#     as_tibble() -&gt; adj_df\n# names(adj_df) &lt;- paste0(1:ncol(adj_df))\n\n# adj_df %&gt;%\n#     group_by(id_1=as.character(row_number())) %&gt;%\n#     gather(id_2, weight, -id_1) %&gt;%\n#     filter(weight != 0)  %&gt;%\n#     mutate(id_2=as.character(id_2), id=paste(id_1, id_2, sep='-')) -&gt; adj_df\n\n# locs = newdata %&gt;% mutate(id=as.character(row_number())) %&gt;% select(id, x1, x2, x3)\n\n# edges=bind_rows(\n# adj_df  %&gt;% inner_join(locs, by=c('id_1'='id')),\n# adj_df  %&gt;% inner_join(locs, by=c('id_2'='id'))\n# ) %&gt;% arrange(id) %&gt;% ungroup()\n\n# pp = ggplot(newdata, aes(x=x1, y=x2)) +\n# geom_line(aes(group=id, size=1), data=edges, color='grey') +\n# geom_point(aes(color=factor(treatment), shape=factor(x3), size=2), alpha=.9) +\n# scale_size_continuous(range=c(1, 3)) +\n# theme_minimal() + theme(legend.position='none')\n\n# print(pp)"
  },
  {
    "objectID": "posts/softblock-demo/index.html#get-precinct-data",
    "href": "posts/softblock-demo/index.html#get-precinct-data",
    "title": "Using SoftBlock to Design an Experiment",
    "section": "Get Precinct data",
    "text": "Get Precinct data\n\nResults Data\n\n\nCode\nurl &lt;- \"http://dl.ncsbe.gov/ENRS/2020_11_03/results_pct_20201103.zip\"\nzip_file &lt;- tempfile(fileext = \".zip\")\ndownload.file(url, zip_file, mode = \"wb\")\nspec = cols(\n  County = col_character(),\n  `Election Date` = col_character(),\n  Precinct = col_character(),\n  `Contest Group ID` = col_double(),\n  `Contest Type` = col_character(),\n  `Contest Name` = col_character(),\n  Choice = col_character(),\n  `Choice Party` = col_character(),\n  `Vote For` = col_double(),\n  `Election Day` = col_double(),\n  `One Stop` = col_double(),\n  `Absentee by Mail` = col_double(),\n  Provisional = col_double(),\n  `Total Votes` = col_double(),\n  `Real Precinct` = col_character(),\n  X16 = col_skip()\n)\nresults &lt;- readr::read_tsv(zip_file, col_types=spec)\n\n\nNew names:\n• `` -&gt; `...16`\n\n\nWarning: The following named parsers don't match the column names: X16\n\n\n\n\nShapefiles\n\n\nCode\nurl = \"https://s3.amazonaws.com/dl.ncsbe.gov/ShapeFiles/Precinct/SBE_PRECINCTS_20201018.zip\"\ntemp &lt;- tempfile()\ntemp2 &lt;- tempfile()\ndownload.file(url, temp)\nunzip(zipfile = temp, exdir = temp2)\nnc_SHP_file &lt;- list.files(temp2, pattern = \".shp$\",full.names=TRUE)\nshapes &lt;- sf::read_sf(nc_SHP_file)"
  },
  {
    "objectID": "posts/softblock-demo/index.html#aggregate-data-and-join",
    "href": "posts/softblock-demo/index.html#aggregate-data-and-join",
    "title": "Using SoftBlock to Design an Experiment",
    "section": "Aggregate Data and Join",
    "text": "Aggregate Data and Join\n\n\nCode\nresults %&gt;%\n    filter(`Real Precinct` == 'Y') %&gt;%\n    group_by(County, Precinct) %&gt;%\n    summarize(\n        total_vote_pres=sum(`Total Votes`[`Contest Name` == 'US PRESIDENT'], na.rm=TRUE),\n        dem_share_pres=sum(`Total Votes`[`Contest Name` == 'US PRESIDENT' & `Choice Party` == 'DEM'], na.rm=TRUE)/total_vote_pres,\n        gop_share_pres=sum(`Total Votes`[`Contest Name` == 'US PRESIDENT' & `Choice Party` == 'REP'], na.rm=TRUE)/total_vote_pres,\n        total_vote_senate=sum(`Total Votes`[`Contest Name` == 'US SENATE'], na.rm=TRUE),\n        dem_share_senate=sum(`Total Votes`[`Contest Name` == 'US SENATE' & `Choice Party` == 'DEM'], na.rm=TRUE)/total_vote_senate,\n        gop_share_senate=sum(`Total Votes`[`Contest Name` == 'US SENATE' & `Choice Party` == 'REP'], na.rm=TRUE)/total_vote_senate,\n        total_vote_gov=sum(`Total Votes`[`Contest Name` == 'NC GOVERNOR'], na.rm=TRUE),\n        dem_share_gov=sum(`Total Votes`[`Contest Name` == 'NC GOVERNOR' & `Choice Party` == 'DEM'], na.rm=TRUE)/total_vote_gov,\n        gop_share_gov=sum(`Total Votes`[`Contest Name` == 'NC GOVERNOR' & `Choice Party` == 'REP'], na.rm=TRUE)/total_vote_gov,\n        total_vote_house=sum(`Total Votes`[grepl('US HOUSE OF REPRESENTATIVES DISTRICT', `Contest Name`)], na.rm=TRUE),\n        dem_share_house=sum(`Total Votes`[grepl('US HOUSE OF REPRESENTATIVES DISTRICT', `Contest Name`) & `Choice Party` == 'DEM'], na.rm=TRUE)/total_vote_house,\n        gop_share_house=sum(`Total Votes`[grepl('US HOUSE OF REPRESENTATIVES DISTRICT', `Contest Name`) & `Choice Party` == 'REP'], na.rm=TRUE)/total_vote_house\n    ) %&gt;% ungroup() -&gt; results_agg\n\n\n`summarise()` has grouped output by 'County'. You can override using the\n`.groups` argument.\n\n\nCode\ninner_join(results_agg, shapes, by=c('County'='county_nam', 'Precinct'='prec_id')) -&gt; df_joined\nDT::datatable(df_joined %&gt;% sample_n(size=100) %&gt;% dplyr::select(-geometry), rownames = FALSE, options=list(scrollX=TRUE, autoWidth = TRUE))"
  },
  {
    "objectID": "posts/softblock-demo/index.html#add-some-geographic-features",
    "href": "posts/softblock-demo/index.html#add-some-geographic-features",
    "title": "Using SoftBlock to Design an Experiment",
    "section": "Add some geographic features",
    "text": "Add some geographic features\n\n\nCode\ndf_joined$geometry %&gt;%\n    st_centroid() %&gt;%\n    st_transform(\"+init=epsg:4326\") %&gt;%\n    st_coordinates() -&gt; latlong\n\n\nWarning in CPL_crs_from_input(x): GDAL Message 1: +init=epsg:XXXX syntax is\ndeprecated. It might return a CRS with a non-EPSG compliant axis order. Further\nmessages of this type will be suppressed.\n\n\nCode\ndf_joined$longitude = latlong[, 'X']\ndf_joined$latitude = latlong[, 'Y']\narea = df_joined$geometry %&gt;% st_transform(\"+init=epsg:4326\") %&gt;% st_area()\ndf_joined$area_km2 = units::drop_units(area) / 1e6 # convert m^2 to km^2\ndf_joined &lt;- df_joined %&gt;% mutate(vote_density_pres = total_vote_pres / area_km2)"
  },
  {
    "objectID": "posts/softblock-demo/index.html#simulate-each-design",
    "href": "posts/softblock-demo/index.html#simulate-each-design",
    "title": "Using SoftBlock to Design an Experiment",
    "section": "Simulate each design",
    "text": "Simulate each design\n\nSoftblockGreedy NeighborsQuickBlock\n\n\n\n\nCode\nsimulate_power = create_power_simulator(softblock_weights, df_joined$treatment_sb, df_joined$dem_share_pres)\nestimate_power_for_effect = function(effect) mean(replicate(25, simulate_power(effect=effect)))\neffects = seq(0, 0.05, length=25)\npower_sb = unlist(purrr::map(effects, estimate_power_for_effect))\n\n\n\n\n\n\nCode\nsimulate_power = create_power_simulator(nn_weights, df_joined$treatment_nn, df_joined$dem_share_pres)\nestimate_power_for_effect = function(effect) mean(replicate(25, simulate_power(effect=effect)))\npower_nn = unlist(purrr::map(effects, estimate_power_for_effect))\n\n\n\n\n\n\nCode\nsimulate_power = create_power_simulator_qb(qb_blocks, df_joined$treatment_qb, df_joined$dem_share_pres)\nestimate_power_for_effect = function(effect) mean(replicate(25, simulate_power(effect=effect)))\neffects = seq(0, 0.05, length=25)\npower_qb = unlist(purrr::map(effects, estimate_power_for_effect))"
  },
  {
    "objectID": "posts/softblock-demo/index.html#power-comparison",
    "href": "posts/softblock-demo/index.html#power-comparison",
    "title": "Using SoftBlock to Design an Experiment",
    "section": "Power Comparison",
    "text": "Power Comparison\n\n\nCode\nggplot(\n    tibble(\n        effects=c(effects, effects, effects),\n        power=c(power_sb, power_nn, power_qb),\n        design=c(rep('SoftBlock', length(power_sb)), rep('Greedy Neighbors', length(power_sb)), rep('QuickBlock', length(power_sb)))\n    ), aes(effects, power, color=design)) + geom_line() +\n    scale_x_continuous('Effect (pp)', labels=scales::percent) +\n    scale_y_continuous(\"Power\", labels=scales::percent) +\n    scale_color_discrete(\"Design\") +\n    theme_minimal()"
  },
  {
    "objectID": "posts/softblock-demo/index.html#average-effects",
    "href": "posts/softblock-demo/index.html#average-effects",
    "title": "Using SoftBlock to Design an Experiment",
    "section": "Average Effects",
    "text": "Average Effects\nThe effect estimates here use the appropriate design-based estimators for each design.\nFirst, I’m going to generate a fake outcome to use. I’ll leave the average effect near zero (0.5pp), but individual effects are random draws from around that vale, but with heterogeneous effects based on democratic vote share in 2020 (i.e. positive effects in democratic precincts and vice versa).\n\n\nCode\ndf_joined$outcome = df_joined$dem_share_pres\ndf_joined$ite = with(df_joined, 0.005 + 0.005 * (plogis((dem_share_pres - median(dem_share_pres)) / sd(dem_share_pres))))\ndf_joined$outcome_sb = with(df_joined, outcome + treatment_sb * ite)\ndf_joined$outcome_nn = with(df_joined, outcome + treatment_nn * ite)\ndf_joined$outcome_qb = with(df_joined, outcome + treatment_qb * ite)\n\n\n\nSoftblockGreedy NeighborsQuickBlock\n\n\n\n\nCode\nestimate_effect = function(W, A, Y) {\n    dL = diag(W)\n    Dinv = (2 * A - 1) / dL\n    x_mat = cbind(A, 1)\n    xlx = t(x_mat) %*% W %*% x_mat\n    bread = MASS::ginv(as.matrix(xlx))\n    coefs = bread %*% t(x_mat) %*% W %*% Y\n    r = diag(drop((Y - (x_mat %*% coefs)) ^ 2))\n    meat = t(x_mat) %*% (W %*% r %*% W) %*% x_mat\n    vcv = bread %*% meat %*% bread\n    list(estimate=coefs[1], std.error=sqrt(vcv[1,1]))\n}\nsb_est = estimate_effect(softblock_weights, df_joined$treatment_sb, df_joined$outcome_sb)\n\n\n\n\n\n\nCode\nnn_est = estimate_effect(nn_weights, df_joined$treatment_nn, df_joined$outcome_nn)\n\n\n\n\n\n\nCode\nresult = quickblock::blocking_estimator(df_joined$outcome_qb, qb_blocks, df_joined$treatment_qb)\nqb_est = list(estimate=result$effects[2,1], std.error=sqrt(result$effect_variances[2,1]))"
  },
  {
    "objectID": "posts/softblock-demo/index.html#plot-average-effects",
    "href": "posts/softblock-demo/index.html#plot-average-effects",
    "title": "Using SoftBlock to Design an Experiment",
    "section": "Plot Average Effects",
    "text": "Plot Average Effects\n\n\nCode\ntibble(\n    estimate=c(sb_est$estimate, nn_est$estimate, qb_est$estimate),\n    std.error=c(sb_est$std.error, nn_est$std.error, qb_est$std.error),\n    design=c(\"SoftBlock\", \"Greedy Neighbors\", \"QuickBlock\")\n) %&gt;%\nggplot(aes(x=design, y=estimate, ymin=estimate-1.96*std.error, ymax=estimate+1.96*std.error)) +\n    geom_pointrange() +\n    scale_x_discrete(\"Design\") +\n    scale_y_continuous(\"ATE (pp)\", labels=scales::percent) +\n    coord_flip() +\n    theme_minimal()"
  },
  {
    "objectID": "posts/softblock-demo/index.html#heterogeneous-effects",
    "href": "posts/softblock-demo/index.html#heterogeneous-effects",
    "title": "Using SoftBlock to Design an Experiment",
    "section": "Heterogeneous Effects",
    "text": "Heterogeneous Effects\nThese effects will be estimated using DR-learner of Kennedy (2020). For simplicity, I will estimate nuisance functions using glmnet.\n\n\nCode\npredict.hte.split = function(x, a, y, s, predict.s=4) {\n    s.pi = (predict.s) %% 4 + 1\n    s.mu = (predict.s + 1) %% 4 + 1\n    s.dr = (predict.s + 2) %% 4 + 1\n    pihat &lt;- predict(cv.glmnet(x[s==s.pi,],a[s==s.pi], family=\"binomial\", nfolds=10), newx=x, type=\"response\", s=\"lambda.min\")\n    mu0hat &lt;- predict(cv.glmnet(x[a==0 & s==s.mu,],y[a==0 & s==s.mu], nfolds=10), newx=x, type=\"response\", s=\"lambda.min\")\n    mu1hat &lt;- predict(cv.glmnet(x[a==1 & s==s.mu,],y[a==1 & s==s.mu], nfolds=10),newx=x, type=\"response\", s=\"lambda.min\")\n    pseudo &lt;- ((a-pihat)/(pihat*(1-pihat)))*(y-a*mu1hat-(1-a)*mu0hat) + mu1hat - mu0hat\n    drl &lt;- predict(cv.glmnet(x[s==s.dr,],pseudo[s==s.dr]),newx=x[s==predict.s, ], s=\"lambda.min\")\n    drl\n}\npredict.hte.crossfit = function(x, a, y) {\n    N = length(a)\n    s = sample(1:4, N, replace=TRUE)\n    hte = rep(NA_real_, N)\n    for (split in 1:4) {\n        hte[s==split] = predict.hte.split(x, a, y, s, predict.s=split)\n    }\n    hte\n}\ncalculate_hte &lt;- function(.data, cols, .treatment='treatment', .outcome='outcome') {\n    expr &lt;- rlang::enquo(cols)\n    pos &lt;- tidyselect::eval_select(expr, data = .data)\n    df_cov &lt;- rlang::set_names(.data[pos], names(pos))\n    cov_mat = scale(model.matrix(~.+0, df_cov))\n    .data$hte = predict.hte.crossfit(cov_mat, .data[[.treatment]], .data[[.outcome]])\n    .data\n}"
  },
  {
    "objectID": "posts/softblock-demo/index.html#estimate-htes",
    "href": "posts/softblock-demo/index.html#estimate-htes",
    "title": "Using SoftBlock to Design an Experiment",
    "section": "Estimate HTEs",
    "text": "Estimate HTEs\n\nSoftblockGreedy NeighborsQuickBlock\n\n\n\n\nCode\ndf_joined %&gt;% calculate_hte(c(\n    longitude, latitude, area_km2, vote_density_pres, # geographic\n    total_vote_pres, # 2020 presidential\n    total_vote_senate, dem_share_senate, gop_share_senate, # 2020 senate\n    total_vote_gov, dem_share_gov, gop_share_gov, # 2020 governor\n    total_vote_house, dem_share_house, gop_share_house # 2020 house\n), .treatment='treatment_sb', .outcome='outcome_sb') %&gt;% rename(hte_sb=hte) -&gt; df_joined\nsummary(df_joined$hte_sb)\n\n\n     Min.   1st Qu.    Median      Mean   3rd Qu.      Max. \n-0.010306  0.004135  0.007536  0.007148  0.009029  0.033580 \n\n\n\n\n\n\nCode\ndf_joined %&gt;% calculate_hte(c(\n    longitude, latitude, area_km2, vote_density_pres, # geographic\n    total_vote_pres, # 2020 presidential\n    total_vote_senate, dem_share_senate, gop_share_senate, # 2020 senate\n    total_vote_gov, dem_share_gov, gop_share_gov, # 2020 governor\n    total_vote_house, dem_share_house, gop_share_house # 2020 house\n), .treatment='treatment_nn', .outcome='outcome_nn') %&gt;% rename(hte_nn=hte) -&gt; df_joined\nsummary(df_joined$hte_nn)\n\n\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n0.005742 0.006825 0.007388 0.007670 0.008596 0.010863 \n\n\n\n\n\n\nCode\ndf_joined %&gt;% calculate_hte(c(\n    longitude, latitude, area_km2, vote_density_pres, # geographic\n    total_vote_pres, # 2020 presidential\n    total_vote_senate, dem_share_senate, gop_share_senate, # 2020 senate\n    total_vote_gov, dem_share_gov, gop_share_gov, # 2020 governor\n    total_vote_house, dem_share_house, gop_share_house # 2020 house\n), .treatment='treatment_qb', .outcome='outcome_qb') %&gt;% rename(hte_qb=hte) -&gt; df_joined\nsummary(df_joined$hte_qb)\n\n\n      Min.    1st Qu.     Median       Mean    3rd Qu.       Max. \n-0.0007862  0.0070832  0.0083612  0.0082249  0.0094074  0.0398552"
  },
  {
    "objectID": "posts/softblock-demo/index.html#plot-hte-distributions",
    "href": "posts/softblock-demo/index.html#plot-hte-distributions",
    "title": "Using SoftBlock to Design an Experiment",
    "section": "Plot HTE distributions",
    "text": "Plot HTE distributions\n\n\nCode\nggplot(df_joined, aes()) +\ngeom_histogram(aes(x=hte_sb, fill='SoftBlock'), bins=50, alpha=0.4) +\ngeom_histogram(aes(x=hte_nn, fill='Greedy Neighbors'), bins=50, alpha=0.4) +\ngeom_histogram(aes(x=hte_qb, fill='QuickBlock'), bins=50, alpha=0.4) +\nscale_x_continuous(\"Effect (pp)\", labels=scales::percent) +\ntheme_minimal()"
  },
  {
    "objectID": "research.html",
    "href": "research.html",
    "title": "Research",
    "section": "",
    "text": "Published\n\n2025\nDrew Dimmery and Kevin Munger. (2025) \"Enough?.\" Observational Studies\n        \n        Preprint\n     \n        \n        Published\n    \n2024\nYan Leng and Drew Dimmery. (2024) \"Calibration of Heterogeneous Treatment Effects in Randomized Experiments.\" Information Systems Research\n        \n        Preprint\n     \n        \n        Published\n    \n2023\nAndrew Guess, Neil Malhotra, Jennifer Pan, Pablo Barberá, Hunt Alcott, Taylor Brown, Adriana Crespo-Tenorio, Drew Dimmery, Deen Freelon, Matthew Gentzkow, Sandra González-Bailón, Edward Kennedy, Young Mie Kim, David Lazer, Devra Moehler, Brendan Nyhan, Carlos Velasco Rivera, Jaime Settle, Daniel Thomas, Emily Thorson, Rebekah Tromble, Arjun Wilkins, Magdalena Wojcieszak, Beixian Xiong, Chad Kiewet de Jong, Annie Franco, Winter Mason, Natalie Jomini Stroud, and Joshua Tucker. (2023) \"How do social media feed algorithms affect attitudes and behavior in an election campaign?.\" Science\n        \n        Data\n     \n        \n        Published\n    \nAndrew Guess, Neil Malhotra, Jennifer Pan, Pablo Barberá, Hunt Alcott, Taylor Brown, Adriana Crespo-Tenorio, Drew Dimmery, Deen Freelon, Matthew Gentzkow, Sandra González-Bailón, Edward Kennedy, Young Mie Kim, David Lazer, Devra Moehler, Brendan Nyhan, Carlos Velasco Rivera, Jaime Settle, Daniel Thomas, Emily Thorson, Rebekah Tromble, Arjun Wilkins, Magdalena Wojcieszak, Beixian Xiong, Chad Kiewet de Jong, Annie Franco, Winter Mason, Natalie Jomini Stroud, and Joshua Tucker. (2023) \"Reshares on social media amplify political news but do not detectably affect beliefs or opinions.\" Science\n        \n        Data\n     \n        \n        Published\n    \nBrendan Nyhan, Jaime Settle, Emily Thorson, Magdalena Wojcieszak, Pablo Barberá, Annie Chen, Hunt Alcott, Taylor Brown, Adriana Crespo-Tenorio, Drew Dimmery, Deen Freelon, Matthew Gentzkow, Sandra González-Bailón, Andrew Guess, Edward Kennedy, Young Mie Kim, David Lazer, Neil Malhotra, Devra Moehler, Jennifer Pan, Daniel Thomas, Rebekah Tromble, Carlos Velasco Rivera, Arjun Wilkins, Beixian Xiong, Chad Kiewet de Jong, Annie Franco, Winter Mason, Natalie Jomini Stroud, and Joshua Tucker. (2023) \"Like-minded sources on Facebook are prevalent but not polarizing.\" Nature\n        \n        Data\n     \n        \n        Published\n    \nMiloš Fišar, Ben Greiner, Christoph Huber, Elena Katok, Ali Ozkes, and Management Science Reproducibility Collaboration. (2023) \"Reproducibility in Management Science.\"\n        \n        Preprint\n     \n        \n        Published\n    \nB. Nyhan, Jaime Settle, Emily A. Thorson, Magdalena Wojcieszak, Pablo Barberá, Annie Y Chen, Hunt Allcott, Taylor Brown, Adriana Crespo-Tenorio, Drew Dimmery, Deen Freelon, M. Gentzkow, Sandra González-Bailón, A. Guess, Edward Kennedy, Y. Kim, David Lazer, Neil Malhotra, D. Moehler, Jennifer Pan, Daniel Robert Thomas, Rebekah Tromble, C. Rivera, Arjun S. Wilkins, Beixian Xiong, Chad Kiewiet de Jonge, Annie Franco, Winter Mason, N. Stroud, and Joshua A. Tucker. (2023) \"Author Correction: Like-minded sources on Facebook are prevalent but not polarizing.\" Nature\n        \n        PDF\n     \n        \n        Published\n    \n2022\nDavid Arbour, Drew Dimmery, Tung Mai, and Anup Rao. (2022) \"Online Balanced Experimental Design.\" ICML\n        \n        Preprint\n     \n        \n        Github\n     \n        \n        Published\n    \nHan Wu, Sarah Tan, Weiwei Li, Mia Garrard, Adam Obeng, Drew Dimmery, Shaun Singh, Hanson Wang, Daniel Jiang, and Eytan Bakshy. (2022) \"Distilling Heterogeneity: From Explanations of Heterogeneous Treatment Effect Models to Interpretable Policies.\"\n        \n        Preprint\n     \n        \n        Published\n    \n2021\nDavid Arbour, Drew Dimmery, and Arjun Sondhi. (2021) \"Permutation Weighting.\" ICML\n        \n        Preprint\n     \n        \n        Published\n    \nDavid Arbour, Drew Dimmery, and Anup Rao. (2021) \"Efficient Balanced Treatment Assignments for Experimentation.\" AISTATS\n        \n        Preprint\n     \n        \n        Github\n     \n        \n        Published\n    \nMy Phan, David Arbour, Drew Dimmery, and Anup Rao. (2021) \"Designing Transportable Experiments Under S-admissability.\" AISTATS\n        \n        Preprint\n     \n        \n        Github\n     \n        \n        Published\n    \n2020\nArjun Sondhi, David Arbour, and Drew Dimmery. (2020) \"Balanced off-policy evaluation in general action spaces.\" AISTATS\n        \n        Preprint\n     \n        \n        Published\n    \n2019\nHongzi Mao, Shannon Chen, Drew Dimmery, Shaun Singh, Drew Blaisdell, Yuandong Tian, Mohammad Alizadeh, and Eytan Bakshy. (2019) \"Real-world Video Adaptation with Reinforcement Learning.\" ICML Workshop - RL4RealLife\n        \n        Preprint\n     \n        \n        Published\n    \nDrew Dimmery, Eytan Bakshy, and Jasjeet Sekhon. (2019) \"Shrinkage Estimators in Online Experiments.\" KDD\n        \n        Preprint\n     \n        \n        Published\n    \n2016\nDrew Dimmery and Andrew Peterson. (2016) \"Shining the Light on Dark Money: Political Spending by Nonprofits.\" RSF: The Russell Sage Foundation Journal of the Social Sciences\n        \n        Published\n    \n2011\nC.T Kullenberg, S.R. Mishra, Drew Dimmery, and the NOMAD Collaboration. (2011) \"A search for single photon events in neutrino interactions.\" Physics Letters B\n        \n        Preprint\n     \n        \n        Published\n    \n\nWorking Papers / Non-archival\n\n2025\nMolly Offer-Westort and Drew Dimmery. (2025) \"Experimentation for Homogenous Policy Change.\"\n        \n        Preprint\n    \nHunt Allcott, Matt Gentzkow, Benjamin Wittenbrink, Juan Carlos Cisneros, Adriana Crespo-Tenorio, Drew Dimmery, Deen Freelon, Sandra González-Bailón, Andrew Guess, Y. Kim, David Lazer, Neil A. Malhotra, D. Moehler, Sameer Nair-Desai, Brendan Nyhan, Jennifer Pan, Jaime Settle, Emily Thorson, Rebekah Tromble, Carlos Velasco, Arjun S. Wilkins, Magdalena Wojcieszak, Annie Franco, Chad Kiewiet de Jonge, Winter Mason, Talia Stroud, and J. Tucker. (2025) \"The Effect of Deactivating Facebook and Instagram on Users’ Emotional State.\"\n        \n        Preprint\n    \nHunt Allcott, Matt Gentzkow, Ro’ee Levy, Adriana Crespo-Tenorio, Natasha Dumas, Winter Mason, D. Moehler, Pablo Barberá, Taylor Brown, Juan Carlos Cisneros, Drew Dimmery, Deen Freelon, Sandra González-Bailón, Andrew Guess, Y. Kim, David Lazer, Neil A. Malhotra, Sameer Nair-Desai, Brendan Nyhan, Ana Carolina Paixao de Queiroz, Jennifer Pan, Jaime Settle, Emily Thorson, Rebekah Tromble, Carlos Velasco, Benjamin Wittenbrink, Magdalena Wojcieszak, Shiqi Yang, Saam Zahedian, Annie Franco, Chad Kiewiet de Jonge, Talia Stroud, and J. Tucker. (2025) \"The Effects of Political Advertising on Facebook and Instagram Before the 2020 US Election.\"\n        \n        Preprint\n    \n2023\nLaura Koesten, Drew Dimmery, Michael Gleicher, and Torsten Moller. (2023) \"Subjective visualization experiences: impact of visual design and experimental design.\"\n        \n        Preprint\n    \n2019\nSam Daulton, Shaun Singh, Vashist Avadhanula, Drew Dimmery, and Eytan Bakshy. (2019) \"Thompson Sampling for Contextual Bandit Problems with Auxiliary Safety Constraints.\" NeurIPS Workshop - Safety and Robustness in Decision Making\n        \n        Preprint"
  },
  {
    "objectID": "posts/quarto-website/index.html",
    "href": "posts/quarto-website/index.html",
    "title": "Quarto for an Academic Website",
    "section": "",
    "text": "I’ve never been good at keeping my website updated. I always go through two different phases of maintenance:\n\nRushing around creating a new website with bells and whistles using whatever the flavor of the month is\nNever updating an existing website\n\nI’m hoping to break out of this cycle, but am currently solidly within Phase 1.\n\nA highlight from my time in Phase 2 was when I forgot to update my DNS and I totally lost control of drewdimmery.com (don’t go there, it has a squatter). I think my website at that time was some Octopress monstrosity. There are a few reasons I think Quarto might help with my vicious circle.\n\nServing static HTML pages is about as easy as it gets\nVery little Quarto-specific syntax to recall (e.g. CLI commands or abstruse markup)\nLots of flexibility (Python / R) in how to generate that static content\nFull programmability means that generation can be based on arbitrary data structures of my choosing\n\nI previously used Hugo Academic for building my website, which was much better than just editing the content directly, but I never remembered the right way to generate a new publication definition (there was a CLI, but I never remembered the syntax). Each publication got its own file describing its details, and I found this quite clunky. I wanted something extremely lightweight: there isn’t much reason for my individual publications to get pages of their own, and I really don’t need a lot of information on each of them. I just want some basic information about each and a set of appropriate links to more details.\nThis post will detail how I’ve set up Quarto to accomplish this task. I’ve nearly completely separated the two main concerns around maintaining an academic website / CV, which to me are data on publications and software from the design elements of how to display them. It’s entirely possible that my particular issues are unique and this post won’t be useful to anyone else. Luckily, the marginal cost of words on the internet is essentially zero (and maybe the marginal value is, too)."
  },
  {
    "objectID": "posts/quarto-website/index.html#data",
    "href": "posts/quarto-website/index.html#data",
    "title": "Quarto for an Academic Website",
    "section": "Data",
    "text": "Data\nI put data about each publication in a basic YAML format:\n\n\nSee example data\n\nsoftblock:\n  title: Efficient Balanced Treatment Assignments for Experimentation\n  authors:\n    - David Arbour\n    - me\n    - Anup Rao\n  year: 2021\n  venue: AISTATS\n  preprint: https://arxiv.org/abs/2010.11332\n  published_url: https://proceedings.mlr.press/v130/arbour21a.html\n  github: https://github.com/ddimmery/softblock\n\nThis is basically like a simplified bibtex entry with more URLs so I can annotate where to find replication materials for a given paper, as well as distinguish between preprints (always freely accessible) versus published versions (not always open access). A convenience that I add in the markup here is referring to myself as me in the author list (which is an ordered list). This allows me to add in extra post-processing to highlight where I sit in the author list.\nSome additional things I considered adding but chose to ignore for a first version:\n\nAn abstract\nA suggested bibtex entry\n\nBoth of these would be easy to add, but I chose to start simpler. I don’t love YAML for entering long blocks of text, which both of these are."
  },
  {
    "objectID": "posts/quarto-website/index.html#formatting",
    "href": "posts/quarto-website/index.html#formatting",
    "title": "Quarto for an Academic Website",
    "section": "Formatting",
    "text": "Formatting\nSince I can write the generation logic for page in Python, this puts me on comfortable ground to hack something together. To knit the above publication data into HTML, I just literally bind together the programmatically generated raw HTML and print it onto the page.\nI do a couple additional useful things in this process: - Separate out working papers or non-archival papers from published work (I make this distinction based on whether I include a published_url field or not). - Order and categorize papers by year - Provide nice Bootstrappy buttons for external links (e.g. to Preprints / Code / etc)\n\n\nSee research.qmd fragment\n\nimport yaml\nfrom IPython.display import display, Markdown, HTML\n\ndef readable_list(_s):\n  if len(_s) &lt; 3:\n    return ' and '.join(map(str, _s))\n  *a, b = _s\n  return f\"{', '.join(map(str, a))}, and {b}\"\n\ndef button(url, str, icon):\n    icon_base = icon[:2]\n    return f\"\"\"&lt;a class=\"btn btn-outline-dark btn-sm\", href=\"{url}\" target=\"_blank\" rel=\"noopener noreferrer\"&gt;\n        &lt;i class=\"{icon_base} {icon}\" role='img' aria-label='{str}'&gt;&lt;/i&gt;\n        {str}\n    &lt;/a&gt;\"\"\"\n\nyaml_data = yaml.safe_load(open(\"papers.yaml\"))\npub_strs = {\"pubs\": {}, \"wps\": {}}\nfor _, data in yaml_data.items():\n    title_str = data[\"title\"]\n    authors = data.get(\"authors\", [\"me\"])\n    authors = [\n        aut if aut != \"me\" else \"&lt;strong&gt;Drew Dimmery&lt;/strong&gt;\" for aut in authors\n    ]\n    author_str = readable_list(authors)\n    year_str = data[\"year\"]\n\n    buttons = []\n    preprint = data.get(\"preprint\")\n    if preprint is not None:\n        buttons.append(button(preprint, \"Preprint\", \"bi-file-earmark-pdf\"))\n\n    github = data.get(\"github\")\n    if github is not None:\n        buttons.append(button(github, \"Github\", \"bi-github\"))\n\n    pub_url = data.get(\"published_url\")\n    venue = data.get(\"venue\")\n    working_paper = pub_url is None\n    \n    pub_str = f'{author_str}. ({year_str}) \"{title_str}.\"'\n\n    if venue is not None:\n        pub_str += f\" &lt;em&gt;{venue}&lt;/em&gt;\"\n\n    if working_paper:\n        if year_str not in pub_strs[\"wps\"]:\n            pub_strs[\"wps\"][year_str] = []\n        pub_strs[\"wps\"][year_str].append(\n            \"&lt;li class='list-group-item'&gt;\" + pub_str + \"&lt;br&gt;\" + \" \".join(buttons) + \"&lt;/li&gt;\"\n        )\n    else:\n        if year_str not in pub_strs[\"pubs\"]:\n            pub_strs[\"pubs\"][year_str] = []\n        buttons.append(button(pub_url, \"Published\", \"ai-archive\"))\n        pub_strs[\"pubs\"][year_str].append(\n            \"&lt;li class='list-group-item'&gt;\" + pub_str + \"&lt;br&gt;\" + \" \".join(buttons) + \"&lt;/li&gt;\"\n        )\n\nI then print this out using the display functions from the IPython module and using the asis chunk option:\n\n\nSee research.qmd fragment\n\nfor year in sorted(pub_strs[\"pubs\"].keys(), reverse=True):\n    display(Markdown(f\"### {year}\" + \"{#\" + f\"published-{year}\" + \"}\"))\n    display(HTML(\n        \"&lt;ul class='list-group list-group-flush'&gt;\" + '\\n'.join(pub_strs[\"pubs\"][year]) + \"&lt;/ul&gt;\"\n    ))\n\nThe full code is on GitHub.\nIt’s worth noting that to get the years to show up in the Table of Contents its necessary to be careful exactly how the content is stuck onto the page. If you don’t use the asis chunk option, you can still get all the right content to show up, but it won’t necessarily appear in the ToC. I also found it necessary to include section-divs: false in the header, or else the output would get wrapped in additional div tags which made it harder to get the right classes in the right divs. There are probably more elegant ways to do all of this.\nI use the same basic setup to populate the Software page, albeit with simpler logic.\n\nAdditions\nI debated adding an abstract that expands out on click (like the code folding above in this post). This would actually be more or less trivial to add using a &lt;details&gt; HTML tag if I wanted to provide the data in the YAML. I’m ignoring this for now because I want to minimize data entry for my future self (and it’s anyway just a click away at the Preprint link)."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Drew Dimmery",
    "section": "",
    "text": "I am the Professor of Data Science for the Common Good at the Hertie School’s Data Science Lab.\nI previously worked on the Adaptive Experimentation team at Facebook Core Data Science in New York, and at the Research Network Data Science at the University of Vienna.\nMy research focuses on the intersection of machine learning and causal inference. I work a lot on experimental design and I think hard about how experimentation can be better and more efficient. I’m also interested in better applying the machinery of machine learning to observational causal inference.\nI’m most interested in methodological research useful for the study of social phenomena."
  },
  {
    "objectID": "software.html",
    "href": "software.html",
    "title": "Software",
    "section": "",
    "text": "tidyhte\ntidyhte provides tidy semantics for estimation of heterogeneous treatment effects through the use of Kennedy’s (n.d.) doubly-robust learner.\nThe goal of tidyhte is to use a sort of “recipe” design. This should (hopefully) make it extremely easy to scale an analysis of HTE from the common single-outcome / single-moderator case to many outcomes and many moderators. The configuration of tidyhte should make it extremely easy to perform the same analysis across many outcomes and for a wide-array of moderators. It’s written to be fairly easy to extend to different models and to add additional diagnostics and ways to output information from a set of HTE estimates.\n\n        \n        Website\n     \n        \n        Github\n    \nregweight\nThe goal of regweight is to make it easy to diagnose a model using Aronow and Samii (2015) regression weights.\nIn short, these weights show which observations are most influential for determining the observed value of a coefficient in a linear regression. If the linear regression is aiming to estimate causal effects, this implies that the OLS estimand may differ from the average treatment effect. These linear regression weights provide, in some sense, the most precise estimate available given a conditioning set (and a linear model). These weights are in expectation the conditional variance of the variable of interest (given the other covariates in the model).\n\n        \n        Website\n     \n        \n        Github\n     \n        \n        Package\n    \nrdd\nOutdated! Users should switch to actively maintained and updated RD tools.\nProvides the tools to undertake estimation in Regression Discontinuity Designs. Both sharp and fuzzy designs are supported. Estimation is accomplished using local linear regression. A provided function will utilize Imbens-Kalyanaraman optimal bandwidth calculation. A function is also included to test the assumption of no-sorting effects.\n\n        \n        Github\n     \n        \n        Package"
  },
  {
    "objectID": "posts/calibration-as-an-hte-diagnostic/index.html",
    "href": "posts/calibration-as-an-hte-diagnostic/index.html",
    "title": "Calibration as an HTE diagnostic",
    "section": "",
    "text": "In an earlier post, I promised that rather than writing threads about papers, I’d talk about them here. That’s what this post is!\nI’ve just had a paper (with Yan Leng) accepted to Information Systems Research titled “Calibration of Heterogeneous Treatment Effects in Randomized Experiments”. The entire motivation of the paper can be summed up in the following chart, showing that the HTEs are not alright:\nThe data undergirding this chart came from a big (and important) experiment at Facebook which was run around 2019 or so. It is sufficiently sensitive that I (still) can’t talk about what the actual intervention was, but one of the big questions was about how much heterogeneity there was in the intervention. It would have been a big deal if a small fraction of people exhibited very large effects, so we needed to look into whether this was the case. We did this by applying some of the rapidly developing literature on ML estimation of heterogeneous effects.\nWe threw machine learning at the problem using a bunch of covariates we collected about users’ behavior. We started with the simplest possible model, the venerable T-learner1. When I was at Facebook, I did a bunch of interviews in which I asked candidates to walk me through their model-building and model-testing process (starting with very simple gut-checks). One of the open-ended questions I would always ask was “how would you know if this particular model was good enough (e.g. to launch to production)?” With ML, you can often give pretty sensible answers to this question depending on what you’re trying to do and thinking carefully about what errors mean2. We found this a surprisingly hard question to answer in the causal setting.\nHow can we have a sense as to whether our HTEs are reasonable? In a standard machine learning setting, you have true labels which you can compare to your estimates to see how well you do at prediction. Even in the case of non-standard prediction models where you only learn about the truth long after you predict, you can use the truth to get a sense of how well you do. A great example of this was when FiveThirtyEight evaluated all of their predictive models they’d ever done. Of course, as you learn in the first lecture of any causal inference class, you never actually observe the true treatment effect: you only have exactly one of the treatment and control potential outcome for a unit.\nThe standard answer in the causal setting is that while you cannot observe individual effects, you can observe average effects. To bring things back to the motivating graph at the top, we started with the very simple exercise of splitting up our data based on quintiles of the estimated HTEs (from the T-learner) and comparing the average model-based estimates against the standard difference-in-means estimates in each quintile.\nThe results were pretty startling, as the ML-based HTEs were radically smaller in magnitude than the known unbiased estimates from a difference-in-means. In short, this plot instantly alerted us to the fact that something was going very wrong. As you can see in the plot, these differences cannot simply be explained away as sampling variation in the difference-in-means. Rather, we found the issue to be that the plugin estimators make a fundamentally incorrect bias-variance tradeoff because they’re modeling response-surfaces rather than treatment effects. If you were to have infinite data, this problem might go away, but we’re ultimately never in asymptopia, and we need some heuristics and diagnostics to help us navigate this tradeoff.\nThe core of our paper is thus to motivate people to look at their data. Have your models actually done a good job of replicating the effects you know you can have some faith in? If not, you need to try and do something else. You can fit a new model (e.g. a DR-learner with tidyhte), or you can use our method to do something basically like Platt scaling, which is what we talk through in the paper: take the aggregated data and find a linear transformation which makes the resulting estimates line up as best they can with the subgroup effects3.\nFor the linear rescaling approach to work well, you have to make a lot of assumptions. The review process was very focused on demonstrating that this calibration procedure improves the underlying HTE estimates4, but I think the best way to think about this is as, fundamentally, a diagnostic method. What we’re doing here is showing you how to look at your data and your model and see whether they align in any way whatsoever5. The ultimate plot we landed on (and recommend) looks like the following, which is read basically like a QQ plot:\nNote that the axes are scaled by the arcsin. In our settings, at least, this was very important, as estimated HTEs tended to have somewhat long tails (you can see that the vast majority of units are estimated to have effects with magnitude less than 1 or so, but there are still some pretty large estimated effects). To be clear, at the tails of the data, the HTE model understates true effects by around 50% (the difference in the blue dashed line defined by the difference-in-means and the red one from the estimated HTEs). When you care about what your intervention is actually doing, that’s an unacceptable error that you must do something to correct.\nIn general, I think finite-sample performance for HTEs is vastly underrated in import. There’s some very weird asymptotic shenanigans that happen in some of these HTE papers that I think should be pushed on more. For instance, did you know that causal forests acquire a lot of their nice properties because asymptotically, each terminal node has a constant treatment effect? How often does our data have remotely constant treatment effects within estimated leaves when heterogeneity actually matters? A lot of how well CF will do on your particular data is going to come down to what your true CATE function looks like (just like a lot of other approaches). Luckily, random forests are generally pretty good models, so I don’t think this invalidates much, but it does suggest you need to think hard about model validation. You shouldn’t just rely on asymptotic results.\nAnother result from our paper I like is that you can exactly characterize the (conditional) bias that you’ll get from fitting a T-learner with ridge regression: it’s purely a function of the regularization parameter: more regularization means more bias6. With a T-learner, you typically manage the bias-variance tradeoff on the response surfaces individually, so if you have a very noisy underlying response function, you’ll get a lot of bias even if your true CATE function is very well behaved! If we want to be serious about estimating HTEs, we should take these kinds of finite sample properties seriously. It’s 100% a finite sample issue, as if your data gets big enough, you won’t need to regularize, so you won’t have any bias.\nIn one of my papers (with David Arbour and Anup Rao), we come at this from a whole different angle, motivating an experimental design based on doing a good job for a particular (knn-based) HTE estimator. We show that one particular view of good design is explicitly trying to minimize the MSE of HTE estimators7. I really like this paper, although it’s admittedly pretty weird8. In particular, I think the connection between experimental design and the Maxcut problem is a fun insight that makes Kallus’s great design paper a little easier to understand.\nAnyway, try talking through all this in a Twitter thread!"
  },
  {
    "objectID": "posts/plagiarism-is-bad/index.html",
    "href": "posts/plagiarism-is-bad/index.html",
    "title": "Plagiarism is bad",
    "section": "",
    "text": "I think it’s important to make the simple point that plagiarism is bad1. I won’t be drawn into specifics of current events which have precipitated this discussion, but there are a lot of academics saying absurd things that I want to publicly disagree with in order to uphold worthwhile norms. This post is less about questions of academic integrity than straightforward questions of what the academic enterprise is."
  },
  {
    "objectID": "posts/plagiarism-is-bad/index.html#what-are-we-doing",
    "href": "posts/plagiarism-is-bad/index.html#what-are-we-doing",
    "title": "Plagiarism is bad",
    "section": "What are we doing?",
    "text": "What are we doing?\nFirst, a digression. Latour and Woolgar (1979) is often credited with the idea that the main thing academics do is publish papers. Our incentive systems are fundamentally built around this idea: if you publish more papers that appear in good journals, you will be more successful. It will cause you to be paid more, have more prestige and, generally, wield more power. I used to think that this meant that what many academics were aiming to do was to simply write more papers. I am increasingly feeling that this is incorrect. Academics want to produce more papers, but they have no interest whatsoever in writing them2.\nIn the course of events, a major perspective has arisen, probably expressed most forcefully like this:\n\n\n\nImage\n\n\nAndrew is a trained economist who does good empirical work in the way trained economists do it. And, as he is telling you here, he does not care that much about his lit review section. To be a bit more blunt, I think the argument is basically that empirical social scientists aren’t writing papers, we’re producing estimates. We produce a couple of these estimates per paper, and they are the only actual information in the paper. We’re forced to surround these numbers with words because of arcane medieval institutions, but we all recognize that this is absurd and nobody cares about the words, only the numbers. The right way to read a paper is to skip all the textual mumbo-jumbo, look at the charts, see the numbers, then maybe read the methods section if their approach isn’t obvious. This is reflected by the way we do meta-analysis: each paper is one dot on a single forest chart.\nTo be clear, I think this is a heinous misrepresentation of what we should think of ourselves as doing. If what we were doing actually mattered, I might even say that I think this view is evil."
  },
  {
    "objectID": "posts/plagiarism-is-bad/index.html#ok-but-what-are-we-doing",
    "href": "posts/plagiarism-is-bad/index.html#ok-but-what-are-we-doing",
    "title": "Plagiarism is bad",
    "section": "Ok, but what are we doing?",
    "text": "Ok, but what are we doing?\nThe work of quantitative social science is to take an irreducibly complex system and reduce some aspect of it down to a 20 page pdf. If we have even the slightest amount of humility, we should instantly understand that this is an impossible task. It is an even more impossible task to reduce that same complex system down to a few quantitative estimates: &lt;20 bits of information3. The reason we have 19.9 pages of text around those numbers is for a few reasons:\n\nbecause we’re trying to be humble by acknowledging all of the uncertainty and context around this impossible task\nbecause we’re trying to acknowledge all of the other human effort that has been put into understanding this aspect of the world before (“standing on the shoulders of giants”)4\n\nThe idea that either objective is well served by copying from the abstract of another person’s work is starkly offensive. It brings to mind a conversation I was having about Ian Hacking the other day. One of the truly remarkable things he does in his writing is create fluent and meaningful reviews of complex work by other people. That it is so easy to read is because he takes the time and effort to translate that other work to the specific questions and topics he’s aiming to consider. This is a monumental effort! It requires a detailed understanding of both the task he has set before himself (e.g. “what do we know about what science is?”) and each of the distinct efforts before that touch on this question — including all the myriad ways that those prior efforts failed to answer the fundamental question, answered a slightly different question or otherwise went astray.\nIs what we’re doing different than this? Fundamentally, I argue it is not. Unless we are completely faithfully replicating an analysis that has been done before5, we ought to have some fresh perspective we are bringing to the table: the relationship between the current work being performed and what has been done in the past will not be the same as pre-existing work. I think the discussion about copying summaries of papers back and forth completely misses the task. A much better term than “literature review” is, I think, “related work”. It highlights that what is being done in the section is highlighting connections between works. Why on earth would you just provide a generic summary of someone else’s work in your paper? A good summary of related work should be opinionated and it should deal in comparisons: it should be providing a contribution in its own right as a way of synthesizing existing work to come to conclusions about what is known, what isn’t known, and what the strengths and weaknesses are of the existing body of work.\nThis is obviously not easy, and for shape-rotators like myself, this isn’t exactly work that is always pleasant. But, like, it’s part of the job, man.\nI suppose the counter-argument is that time spent working on related work is time not spent working on the quantitative estimates. That’s true, but there are two major counterarguments: (i) the thing we’re producing is the paper, and the related work section is a meaningful fraction of that: Spend the time to make it better. (ii) Specialization means not everyone has to spread their time among all sections in the paper. If you’re doing a shitty job on one part of a paper, add a coauthor that can do a better job on that part6."
  },
  {
    "objectID": "posts/plagiarism-is-bad/index.html#in-defense-of-thinking",
    "href": "posts/plagiarism-is-bad/index.html#in-defense-of-thinking",
    "title": "Plagiarism is bad",
    "section": "In defense of thinking",
    "text": "In defense of thinking\nMy point is simple. For historically contingent reasons (not necessarily good reasons), academia has chosen that the primary way that we should communicate is through the medium of the academic paper. Given this choice, I think it’s pretty important that we actually care about what we can communicate through this medium, which has the capacity to share much more than a few quantitative estimates.\nAn example given by Matt Blackwell7 is illustrative:\n\n\n\nImage\n\n\nThis was particularly funny to me because I just wrote a paper where this common boilerplate sentence was probably my most highly edited one through the entire revision process. I will soon publish a broader blogpost around this (now forthcoming) paper, but first I will quote the relevant section:\n\nThis relationship closely accords with the definition of calibration in classiﬁcation and regression problems (Kuleshov et al. 2018), with the added challenge resulting from the fundamental problem of causal inference: Labels (i.e., ITEs) are never observed (Holland 1986).\n\nThe bolded sentence roughly accords with Matt’s example of how people usually cite Holland (1986). Crucially, the boilerplate version is not useful for the specific point I wanted to make in this selection! I specifically wanted to indicate (by my use of the very machine-learning term “label” when referring to individual treatment effects) the challenge this well-trodden conceptual problem poses for simple supervised learning. This required different words. It’s my contention that this is almost always the case. If you find yourself repeating something that has been said many times before, maybe you don’t actually need to say it!\nWe should therefore not normalize the reuse of other people’s words (suited to one particular context, audience and purpose) and re-use them for our own (different!!) context, audience and purpose. I think it is both true that we have sleep-walked into reusing text pretty commonly, as in Matt’s example, and also that we should not do that."
  },
  {
    "objectID": "posts/plagiarism-is-bad/index.html#climbing-off-the-high-horse",
    "href": "posts/plagiarism-is-bad/index.html#climbing-off-the-high-horse",
    "title": "Plagiarism is bad",
    "section": "Climbing off the high horse",
    "text": "Climbing off the high horse\nI recognize that it isn’t always possible to achieve the kind of writing that I’m talking about here. We live in a fallen world. I don’t particularly consider myself a good writer or editor, and everyone faces constraints that lead us to behave in ways that we don’t think are fully consistent with our values. But that doesn’t mean that it’s good or neutral when we make those compromises. It is bad. Plagiarism is bad.\nThe real problem here is, of course, systemic. We are rewarded for producing as many papers as possible, not particularly for having a well crafted discussion on related work. I do, however, want to at least reinforce the norm here: you should write your own papers, even the parts you don’t like (or get someone else to write them). You should care about writing those sections well, even if they aren’t your favorite part of the paper. They are all part of the work you are signing your name to.\nWe should not make light of the actual work we do as academics by implying that it is fine to reuse text that is not our own. It is not."
  },
  {
    "objectID": "posts/stop-looking-for-the-next-twitter/index.html",
    "href": "posts/stop-looking-for-the-next-twitter/index.html",
    "title": "Stop looking for the next Twitter",
    "section": "",
    "text": "Everyone seems to agree now that Twitter/X is bad. They’re right. But the desire to recapture some idyllic ahistorical version of Twitter is wrongheaded. The nature of the platform has serious problems that cannot be solved by a different owner, CEO or by changes in content moderation/ranking. The medium is the message, and the medium is, overall, pretty bad for a lot of what we use it for.\nI’m going to trace through my argument around why the design of microblogging platforms are bad and then sketch out how I’m personally choosing to engage with them in light of this badness."
  },
  {
    "objectID": "posts/stop-looking-for-the-next-twitter/index.html#all-microblogging-is-fundamentally-the-same",
    "href": "posts/stop-looking-for-the-next-twitter/index.html#all-microblogging-is-fundamentally-the-same",
    "title": "Stop looking for the next Twitter",
    "section": "All microblogging is fundamentally the same",
    "text": "All microblogging is fundamentally the same\nThere are differences between microblogging sites (I’m primarily thinking about Twitter/X, BlueSky, Mastodon and Threads, but I imagine anything else basically fits the same mold). Nevertheless, I think the core properties are basically the following:\n\nShort text snippets. I think this is definitional. If it weren’t based around this, it wouldn’t be microblogging.\nVirality. Content gets out of the small community in which it’s posted. This is fun (at first) for the poster and fun for the viewer. The nature of a viral platform means when you see stuff from outside your community, it has already been vetted, so it is almost definitionally ‘highly engaging’ for some highly specific definition of those words.\nSimple engagement possibilities. There are very simple, easy ways to engage with the content you see. A “like” or a “reskeet” is just a click of the mouse away. You don’t have to think very hard or construct a complete and cogent thought in order to “engage” with what you see1.\nA “feed” of posts. You are deluged with an infinitely scrolling progression of more content (making you infinitely content, right?). There are a lot of other features that matter on the margin. Exactly how long is a post? Can you Quote-tweet / Retweet? What content is allowed / promoted? This stuff matters, but I don’t think it really gets at the core of what microblogging is. (As an aside, I think we as social scientists are much better at testing the differences between possible affordances within a single platform than we are at testing the big meaningful differences between platforms).\n\nThe thing that I find really clear about this is that these affordances are not built to enable deep and thoughtful exchange of ideas. They’re built to enable surface-level engagement that you’ll come back for again and again."
  },
  {
    "objectID": "posts/stop-looking-for-the-next-twitter/index.html#some-microblogging-is-different-but-not-better",
    "href": "posts/stop-looking-for-the-next-twitter/index.html#some-microblogging-is-different-but-not-better",
    "title": "Stop looking for the next Twitter",
    "section": "Some microblogging is different (but not better)",
    "text": "Some microblogging is different (but not better)\nI think Jack may have learned exactly the wrong lessons from Twitter. It seems that the AT protocol which undergirds BlueSky is fundamentally built around the idea that Jack shouldn’t have to be the person that everyone gets mad at. You don’t like how your feed is ranked? Not his fault, that’s a different layer than he controls. Don’t like how content moderation works? Well, you should be on a different application with different rules. This goes to extremes with platforms like Mastodon, where some servers won’t do any moderation whatsoever, while others will ban you aggressively for not giving adequate content warnings. This is different than other microblogging services, but I don’t think there’s much of a way to say it’s better. For instance, the massive CSAM problems there.\nHow this has worked in practice with AT/BlueSky is kind of different. BlueSky is the only real application built on AT, which means that Jack doesn’t just get to sit around dealing with AT issues, he has to deal with BlueSky issues, too. All of a sudden (and despite his original intentions), he’s having to manage an actual platform rather than just a protocol. This means that BlueSky is (slowly) developing content moderation standards, some of which are actually implemented at the protocol layer (e.g. in AT rather than in BlueSky). As I understand, CSAM, for instance, is a protocol-level concern for AT, it is not federated out to individual applications. I can’t see how this won’t continue to be a giant wellspring of conflicts."
  },
  {
    "objectID": "posts/stop-looking-for-the-next-twitter/index.html#what-are-we-doing-on-here",
    "href": "posts/stop-looking-for-the-next-twitter/index.html#what-are-we-doing-on-here",
    "title": "Stop looking for the next Twitter",
    "section": "What are we doing On Here?",
    "text": "What are we doing On Here?\nThere are two main things I think happen on microblogs:\n\nDiscovery. There’s nothing I’ve seen that’s better at delivering interesting papers and news items to the front of my face (i.e. “pointing at things”). Microblogging is extremely effective at making serendipitous connections. Getting pointers to things of interest from people within a hop or two of you in a network which you form according to your whims is extremely powerful!\nJokes/entertainment. There’s a reason that dril is the greatest poster on microblogs. One-liners are a classic joke format, and they’re enjoyable to consume on a feed (perhaps especially a feed that also has a lot of self-important academics on it). What I think doesn’t happen, is anything very much like “discussion”. Discussion requires grace between interlocutors, which is hard to extend given the threat of virality inherent in the platform. Discussion requires context, which is destroyed with virality and its associated context collapse. And important discussions require depth, which is not really possible in short snippets of text. I also think that privacy is often crucial to good discussion2. Real privacy is anathema to virality: you can’t really have both. When discussions are ephemeral and low-cost its much easier to try out ideas to see if they seem compelling and convincing.\n\nThe easiest discussions to have on microblogs are ones in which the interlocutors already agree with one another and share full context (or which are just pointing to something and saying, essentially “good” or “bad”). In this case, a few words are all it takes to explain what one means and make one’s position clear. As discussions require more nuance and rely on context that is not shared (i.e. on more complex topics), then the goal should not be to write in chunks of a few hundred characters.\nI think that a lot of the appreciation of tweet-length communication is a reflection of a failure of academic writing. For a lot of research, the first time someone takes the time to make it broadly accessible is when a tweet-thread is written out3. I’ll bite the bullet, and freely admit that was true for me with our ICML paper about online balanced experimental design. The version we submitted (and which was accepted) was essentially just math. The paper’s structure was, essentially, introduction → math → simulations → the end. Reviewers didn’t love it, but they were intimidated by it4. We cleaned it up for the camera-ready version (yes, that’s the easy-to-digest version), but the first time we really thought about how to communicate our results to a broader audience was when I wrote about it for Twitter. That’s bad!5 While one solution to this problem is to just write papers that are more accessible, the incentives are currently entirely against this. Being rigorous but difficult to understand is, often, a very good move if your primary incentive is publication.\nTwitter, however, has incentives which at least preference some amount of accessibility. If you want your work to be shared and consumed, you have to hook people and encourage them to “engage.” In contrast to typical academicese, I think this is a nice change of pace, and is one of the reason I sometimes appreciate them. “Engagement,” however, is quite different than “accessible.” To be accessible, I think you want things like easy hyperlinks/citation, formatting, figures, quotations and, yes, more than a couple hundred characters. Maybe you also want to throw in some code or math as a treat! We can obviously do better than tweets for this."
  },
  {
    "objectID": "posts/stop-looking-for-the-next-twitter/index.html#what-am-i-doing",
    "href": "posts/stop-looking-for-the-next-twitter/index.html#what-am-i-doing",
    "title": "Stop looking for the next Twitter",
    "section": "What am I doing?",
    "text": "What am I doing?\nI’m treating microblogging for what it is. I’m essentially indifferent between services, as I don’t think the differences are that meaningful relative to what’s the same. I enjoy finding things on these platforms, so I will peruse them, but I will not expect discussion, and I won’t use them as if I do. Since I think threads about new work are not a particularly good way to provide digestible versions of papers, I will avoid making them. I don’t want to push for “engagement” with my work; I want people to understand it and its context. Instead, I will share new work, and (if appropriate) will link to a longer-form accessible introduction for that work in this space (or somewhere like it)6. In short, I will try to make it easy to discover my work on microblogs, but I will not attempt to explain my work in that setting. The platform is not conducive to such explanation. I may also make jokes, but, well, most of those are for the groupchats where I can be spicy.\nI think this is ultimately the kind of recommendation Neil Postman makes about television in Amusing Ourselves to Death, too: TV is fantastic entertainment device, but don’t mistake it for something that it’s not. Don’t treat it as a medium of explanation or of news or of education. What it does is entertain.\nMicroblogging platforms are not built for good discussion. I will avoid a poor facsimile of conversation there, and have conversations in places where those conversations can be at their best. For me, this means that my conversation happens outside of the “clearnet”. I have a variety of WhatsApp / Slack / Discord groups for talking with collaborators or just randomers with shared interests. This is, I believe, a much more productive approach than trying to make microblogging into something that it can’t ever really do well. In these environments, you can set up affordances to enable the kind of discussion you want: you can make it easy to share LaTeX, or code. You can make spaces for highly moderated discussions or free-flowing sharing of memes. But most fundamentally, you can retain so much more control and privacy7."
  },
  {
    "objectID": "posts/stop-looking-for-the-next-twitter/index.html#build-alternative-communities",
    "href": "posts/stop-looking-for-the-next-twitter/index.html#build-alternative-communities",
    "title": "Stop looking for the next Twitter",
    "section": "Build alternative communities!",
    "text": "Build alternative communities!\nAbout a year ago I started up a Discord for discussion about experimentation which ultimately hasn’t seen much discussion in it. I’m continuing to hang out there, so if anyone wants somewhere outside of the clearnet to talk about experiments, feel free to join! Or come there to tell me about why this post is bad.\nBut also, use microblogs to create your own semi-private communities and make them easy to discover! The magic of the internet is that it’s all code and you can create whatever you want within those (weak) bounds. There are a ton of tools out there to create whatever kind of community you want, and you don’t need to troll around for “the next Twitter.” Twitter wasn’t ever all that good, and you can just go ahead and make the better community that you’d rather see! Many of those communities won’t work8, and that’s totally fine; the process of figuring out what works is how we find a space for online discussion that’s better than Twitter, which is what we should aspire to, anyway.\n\n\n\nImage\n\n\nWhat AI thinks “a twitter-like website” looks like. It isn’t wrong.Thank you for reading Drew’s News. Now please microblog this in a long thread."
  },
  {
    "objectID": "posts/a-blueprint-for-the-regulation-of-tech/index.html",
    "href": "posts/a-blueprint-for-the-regulation-of-tech/index.html",
    "title": "A Blueprint for the Regulation of Tech",
    "section": "",
    "text": "The US2020 Facebook and Instagram Election Project (US2020 from here out), the first four papers of which were recently published in Science and Nature, can be the blueprint for meaningful regulation of large online platforms. It’s crucial not just to be satisfied in access to data, but in access to rigorous tests of how changes on platforms affect society.\nThe key connection is to avoid examining the project through a purely academic lens about academic research and the knowledge gained through the studies.\nThe news post at Science by Kai Kupferschmidt focuses on this dimension, with input from Joe Bak-Coleman’s saying that “This is not how research on the potential dangers of social media should be conducted”. He focuses on the research and the project as a model for gaining academic knowledge.\nThe commentary by project rapporteur Michael Wagner focused, too, on the project as largely academic in nature. Wagner gets one element of the project exactly correct from this dimension: It requires the goodwill of Meta to work correctly, which makes a difficult model for future scholarly work.\nMeta wants the project interpreted solely as the results of academic research based on Nick Clegg’s news post: “Its findings will be hugely valuable to us, and we hope they will also help policymakers as they shape the rules of the road for the internet”. The findings of these studies should shape discourse on regulation, and not the structure of the collaboration itself. These findings are (overall) pretty positive for Meta. They show that algorithms are powerful (they change a lot about on-platform behavior), but they aren’t scary (they don’t swing elections). There are subtleties here, and I don’t mean to get into a detailed accounting of the exact substantive results, but I think this is the reading of the Project Meta wants to push to advertisers (the former story) and to regulators (the latter one).\nBrandon Silverman gets nearly to the quick: “That’s why ultimately we need regulation if we want more of this sort of thing and for me, that represents one of the biggest promises of the Digital Services Act[…]” We should think about regulation as a way to free this kind of information from tech companies. But he doesn’t go nearly far enough. A model of data-sharing (proposed in the next tweet) does not go far enough, and it sets up perverse incentives for companies to neglect rigorous measurements of changes that might place them in a negative light.\nMy argument in this piece is that the structure of collaboration of US2020—sophisticated experts telling online platforms what they must measure about societal impact—should be the future of the regulation of the internet. Don’t simply accept the framing that Nick Clegg pushes, that we should take these findings and make policies based solely on them (implicitly assuming they are the only evidence we might get). The way platforms work is constantly changing: we should not seek perfect generalizability of the findings, we should instead seek systems which allow us to continue to probe and measure how these platforms work flexibly, as they change."
  },
  {
    "objectID": "posts/a-blueprint-for-the-regulation-of-tech/index.html#what-do-we-want-to-measure",
    "href": "posts/a-blueprint-for-the-regulation-of-tech/index.html#what-do-we-want-to-measure",
    "title": "A Blueprint for the Regulation of Tech",
    "section": "What do we want to measure?",
    "text": "What do we want to measure?\nMany of the most important questions we have about tech’s relationship to society are counterfactuals: If Facebook were otherwise identical but ranked Feed chronologically rather than by an algorithm, would there be more or less hate? If YouTube made different choices in recommendations, would it make people less extreme? A counterfactual, fundamentally, asks what would happen were the world just a little bit different. When thinking about the regulation of technology, we’re asking questions like these: if Facebook were to operate differently in some way, would society be better or would it be worse? This embeds two questions: One is a scientific question: what would society be like if Facebook were different? The second is a question of values: Would that counterfactual society be better or worse? \nThe scientific question can be answered through randomized control trials (RCTs). Such RCTs, however, can only truly be implemented by and within these large tech platforms, so under the status quo the question of values can only be answered by people within those walls. Engineers, data scientists, managers and executives of tech companies: mostly good hearted people, but people who work within institutions that require they think about the good of the platform rather than the good of society.\nI worked within Meta for years on improving the process through which the company evaluated potential counterfactual versions of itself – in other words, testing changes through A/B tests. The entire business of software development is to try lots of stuff out but only keep the things that work towards the business’s goals. These goals are expressed in numbers (“KPIs” or “metrics”). My team built machine learning tools to realize these business goals. When business values change, so too, can the platform. Look no further than Facebook’s 2018 pivot to “meaningful social interactions”. Once new numbers are chosen as the target, machine learning gears turn and the platform (in many ways a black box to everyone) pivots on a dime to optimize these new metrics.\nMany of these business values are widely held societal norms: nobody in tech wants more spam or child sexual abuse material, for instance. As such, companies develop sophisticated (albeit imperfect) systems for detecting and removing such content and ensuring that it is not distributed widely. Other metrics, like the time spent on a platform, are less clear cut: essentially all internet platforms prize this as a KPI, but it does not necessarily align with what is best for society.\nDemocratic societies have systems for answering questions of values: we elect representatives who enact policies so that society embodies the values we care about. When such values are contentious, there is conflict, of course, but there is a peaceful process for resolving that conflict through our institutions. Tech lacks the mechanism to ensure that it embodies societal values: even market pressure isn’t straightforward, since usage is free thanks to advertising."
  },
  {
    "objectID": "posts/a-blueprint-for-the-regulation-of-tech/index.html#who-gets-to-measure-them",
    "href": "posts/a-blueprint-for-the-regulation-of-tech/index.html#who-gets-to-measure-them",
    "title": "A Blueprint for the Regulation of Tech",
    "section": "Who gets to measure them?",
    "text": "Who gets to measure them?\nThe only real counterfactual evidence about tech’s effects is controlled by tech itself. This is the promise of US2020, a project I worked on while at Meta. The unique power of this project is that it reveals the counterfactuals which—but for the press of a button—Facebook can decide to make reality. Asking ‘Should Facebook remove algorithmic ranking?’ contains both a question of science and a question of values – and this project answers the question of science, showing precisely what would happen if Facebook did no algorithmic ranking on Feed. As a result, the question of values is finally exposed to democratic scrutiny. Do we measure the right things to effectively answer that question? The way forward can only be an iterative refinement of asking the question and trying to improve the answers we get.\nIn short, the Election Project is a blueprint for the effective regulation of technology companies. The approach is straightforward and consistent with the language of Chapter III, Section 5 of the Digital Services Act. Regulators can require that for important, societally relevant changes, Very Large Online Platforms must run and report the results of RCTs. These are far more informative than audits of the processes used in these platforms or providing data access without counterfactuals, or trying to explain the precise details of how underlying machine learning systems work (which are mostly black boxes to everyone internally, too). To reiterate: in general, many of the most important systems at VLOPs are not truly “understood” by those who create them. Rather, they are refined by measuring what they do, and launching the changes that push those measurements in a direction that those measurements deem positive.\nPhilipp Lorenz-Spreen has already compellingly argued that Article 40 of the DSA is a way to free the results of internal A/B tests to the public. Tech companies are already running lots of A/B tests, so maybe we can just require them to share those results! This is a very positive step, but it isn’t enough. The problem with limiting ourselves to these tests is that platforms can simply stop doing them on issues they feel present a regulation risk. We’re back in the same place we started where our ability to understand counterfactuals is dependent on the platforms’ goodwill.\nPlatforms already limit what is measured internally for this reason. A direct example of this is race in the US. Put simply, Meta works hard to limit what it knows about race in order to preclude legal risks like it faced around housing discrimination in 2019. When I left Meta, the primary way concerns about disparate impact were examined were through zip-code level demographics. This process ensures that individual users’ race was not explicitly inferred, so race cannot be explicitly taken into account in decisions. To be clear, this does not mean that systems do not have disparate racial impacts. It does, however, make it very hard to measure when those disparate impacts might exist.\nThere are also precedents for requirements to run RCTs, such as with the UK’s Competition and Markets Authority requirement that Google test changes to third-party cookies. The logic here was that Google’s change to how cookies work might lead to big changes in how the advertising market works. Without understanding that impact, the regulator can not make an informed decision about whether it was anti-competitive or not. Hence, require Google to measure the impact, since they’re the only people who can.\nMore traditionally, this is key to the FDA’s regulation of food and medicine. The regulator is not generally performing the clinical trials to demonstrate safety itself: it instead requires that the would-be producer carry out the required tests, and ensures that those tests meet the standards of scientific rigor necessary to ensure safety. We think it’s important to verify that drugs meet at least a minimal standard before pushing them out broadly to all citizens. The same should be true for large changes to internet platforms. Note that the structure of clinical trials under the FDA would be exactly subject to the same critiques that are being made of US2020. \nThe point is not who is actually running the test, but who is defining what particular tests must be conducted and how resulting outcomes must be measured. As an insider to this process, let me be very clear: US2020 took incredible care that external academics understood in detail the process by which concepts were operationalized and measured. A lot of the back-and-forth in the collaboration was specifically about this: academics didn’t understand how data is collected and stored internally, and internal researchers needed clear operationalizations to measure the concepts requested. I think I saw someone throw out that there were around 1000 variables to figure this out for.\nThe DSA provides an avenue for this kind of accountability through counterfactual evidence, but it isn’t through Article 40. Instead, it’s through Article 34. Allow me to quote:\n\nProviders of very large online platforms and of very large online search engines shall diligently identify, analyse and assess any systemic risks in the Union stemming from the design or functioning of their service and its related systems, including algorithmic systems, or from the use made of their services.\n\nBy suggesting that these risks arise from the design or functioning of the service, this sentence invokes counterfactuals. The question at play is whether platform design causes these risks. The only valid way to assess this is through counterfactuals as measured through randomized control trials. The counterfactual risk is what VLOPs should demonstrate in their risk assessments. I recognize that this plain-english reading is not formal legal analysis (and I am not a lawyer). Given the latitude of regulators to choose how this law should be interpreted, I hope that they do so in the way that will give them actual answers to the societally important questions of counterfactual risk. US2020 shows them how they can do that. Unfortunately, based on the Commission’s current example of how to apply the risk management framework, this is not likely to be the approach taken to interpreting the DSA."
  },
  {
    "objectID": "posts/a-blueprint-for-the-regulation-of-tech/index.html#when-are-they-measured",
    "href": "posts/a-blueprint-for-the-regulation-of-tech/index.html#when-are-they-measured",
    "title": "A Blueprint for the Regulation of Tech",
    "section": "When are they measured?",
    "text": "When are they measured?\nTime is a crucial element of this story. The number one complaint by everyone involved in US2020 is the time it took to make it happen. As Kevin Munger emphasizes, it was 32 months after the US2020 studies were actually performed that they were published. Part of that delay was because of peer review (note that the chronological feed paper was originally received by Science on March 7, 2022, but was only published on July 27, 2023. The paper was definitely improved in that time, but it goes without saying that nothing about the intervention or outcome measurement could have been improved or changed as a result of this peer review. It had already happened.\nSurely nothing changed about Facebook in that time, right? Well, shortly after submission of chrono-feed, on July 27, 2022, Mark Zuckerberg made an announcement on the Q2 earnings call: “Right now, about 15% of content in a person’s Facebook feed and a little more than that of their Instagram feed is recommended by our AI from people, groups, or accounts that you don’t follow. We expect these numbers to more than double by the end of next year.” Shit. It’s at this point that I’ll note we found clear heterogeneity of the effects of chronological feed on variables like how much uncivil content and slurs show up on Feed, as well as on-platform political behavior on the dimension of inventory size. When there’s more content Feed could show you, there’s more potential for Feed algorithms to change what you see.\n\n\n\nImage\n\n\nLarger inventory is associated with larger effects on on-platform political behavior.Kevin and I have a new working paper showing exactly how poorly agnostic approaches to generalizability fare when reality refuses to stay fixed. When reality is changing at internet speed, then it’s necessary that our knowledge generation process matches that speed. US2020 could do better on this. A lot of work had to be done to mitigate the risks that were inherent in the project as the first of its kind. There was a whole software infrastructure that had to be built from the ground up for protecting privacy, which was done at a pretty incredible speed. There was also a lot of work to build up the internal processes to make everything work both within the collaboration and within Meta (e.g. legal and privacy reviews to make sure user data was appropriately protected). And as stated above, there was a whole arduous process of defining meaningful operationalizations of everything: to a real extent, the scientific language of internal and external researchers wasn’t exactly the same, so translation was required.\nIf we’re thinking about an ongoing regulatory process, however, a lot of these concerns go away: an infrastructure and set of systems will be set up (by necessity) to streamline processes. Variable definitions will be negotiated over time, so that’s largely just a fixed cost per variable. If the output is regulatory rather than academic, the results wouldn’t need to be gated by the fickle peer review process (just government bureaucracy!).\nDefining appropriate implementation of the regulations, too, must be a negotiated and long-term process. Meta has spent years defining specific internal metrics to understand the best dimensions of platform behavior to measure for business purposes. We cannot expect US2020 to have gotten the right societally important dimensions correct on the first try. DSA requires platforms to measure risks “on civic discourse and electoral processes, and public security”. How should these be operationalized? This can only be an iterative process as we see what kinds of measurements are both informative about what we care about in the world and sensitive to the kinds of changes VLOPs make. Iteration on the US2020 model is how to begin this process of refinement."
  },
  {
    "objectID": "posts/a-blueprint-for-the-regulation-of-tech/index.html#wrapping-it-up",
    "href": "posts/a-blueprint-for-the-regulation-of-tech/index.html#wrapping-it-up",
    "title": "A Blueprint for the Regulation of Tech",
    "section": "Wrapping it up",
    "text": "Wrapping it up\nIf we want to truly understand a world where tech is different in some way, we must change it in that way and measure what happens as a result. This is how Meta ensures Feed aligns with its business values, and this is the only way to ensure Feed – alongside all other aspects of online platforms – aligns with our societal values, as well. This may require platforms to run RCTs they would not have already run: we may imagine changes that they have not previously tested. At the very least, it will almost certainly be necessary to require that they measure outcomes they might not otherwise measure. In the US, for instance, this might mean requiring that they collect data on race that they are otherwise reluctant to hold.\nUS2020 has many of the features that are necessary for this kind of counterfactual accountability. It provides rigorous evidence of how Meta’s products would work if we were to change them. The RCTs were designed by external academics with full control rights, whose incentive is not to improve Meta’s bottom-line, but to expose answers to societally relevant questions. Meta had no substantial freedom to say no to such requests. It was not fast enough, nor was it large enough scale (e.g. it was only in the US). Regulators don’t currently have the expertise in-house to do the work that these academics did in asking good questions and co-designing studies to answer them. This is an important problem to solve, but other fields (like the FDA) have shown that such expertise can be acquired by regulatory agencies.\nThe biggest problem with US2020 is that it only exists out of Meta’s goodwill. And given how expensive it was for Meta, it’s not clear how much of that there is left. Look through the author list for the Meta employees – most worked primarily on this project for the last 3 years, and lots of other engineers and data scientists contributed substantial time as well. These people are all superstars and just their direct compensation over that time is a huge commitment from Meta: easily millions of dollars, likely tens of millions. Given recent belt-tightening, I wouldn’t count on Meta independently finding this a good tradeoff solely in order to be transparent. If we want to guarantee this kind of transparency from internet platforms, then we shouldn’t just hope that they will keep choosing to bring us rigorous counterfactual evaluation. Government should require it.\nThink other people should read this? Feel free to share:"
  },
  {
    "objectID": "posts/what-was-us2020/index.html",
    "href": "posts/what-was-us2020/index.html",
    "title": "What was US2020?",
    "section": "",
    "text": "The US 2020 Facebook and Instagram Election Project (US2020) was a sprawling project with dozens of people involved at various levels: like little ol’ me (on the Meta side). Meta spent more than $20 million on it and committed the time of numerous experienced data scientists and engineers. It was huge. But it was also very small compared to the size of Meta.1 In this post, I want to reflect on what, exactly, it was, particularly in light of the eLetters to one of its marquee studies and Kevin Munger’s nice article asking what we learned from the project. The fairest reading of the project is the result of a dialogue between internal and external researchers, so that’s the direction I’m going to work towards.\nThe idea here is that I want to try to develop a theory of what this project was2. That is, why does the collaboration exist? Should it exist? If and when we want to do Big Social Science like this, what will we be doing? What should we be doing?\nTo make the argument that US2020 is best understood as a dialogue, I need to start by talking through what and how large Facebook (the company) was and how experimentation works in that context which is defined by the scope of the collective endeavor that is Facebook (the platform). After that I can address what I see as fundamental misperceptions about the nature of US2020: the idea that the project was either industrial research or independent research. It wasn’t, and trying to fit it into one of these molds leads to many confused discussions about the research.\nIndependence in future projects should not be the goal, but this does not mean that we need to sacrifice reliability or trustworthiness. Quite the opposite. The very human trustworthiness of the project is as a result of the combination of the non-independence (of the internal researchers) and independence (of the academic researchers). The* tension* between the two of these was what made this project valuable. We can accept that this tension will exist, but resolve much of it through the use of pre-analysis plans. This will better allow us to express the agency and expertise of everyone involved in the project and, hopefully, make future projects more likely to exist and be better.\nSo, where to start?"
  },
  {
    "objectID": "posts/what-was-us2020/index.html#what-was-facebook",
    "href": "posts/what-was-us2020/index.html#what-was-facebook",
    "title": "What was US2020?",
    "section": "What was Facebook?",
    "text": "What was Facebook?\n\n\n\nImage\n\n\nFacebook was an internet technology company from 2004 to 2021. It no longer exists, having been succeeded by “Meta”. At its peak in 2021, it had70,000 employees and 118 billion in revenue, truly staggering numbers that are difficult to comprehend. Around half of those employees were in technical roles3: they directly worked on changing and improving the functioning of the services that Facebook offered. Can you imagine working on a project with 30,000 other people? It’s critical to understand that this is part of what it means to work at Facebook. And it really felt like this, sometimes! Internal tooling changed often (because people were actively working on it!), the use of a Monorepo meant that all engineers pushed code to the same repository and there were meaningful connections in code between projects. To land a commit at Facebook famously only required you to get one other engineer to sign off on it4. The number of commits to the repository was so large that you couldn’t expect to have the same ground truth from one second to the next5. For a sense of the speed at which Facebook changed, read this post detailing some of the deployment systems Facebook used written by Chuck Rossi, a famous6 release engineering guru at the company."
  },
  {
    "objectID": "posts/what-was-us2020/index.html#experimentation-at-scale",
    "href": "posts/what-was-us2020/index.html#experimentation-at-scale",
    "title": "What was US2020?",
    "section": "Experimentation at scale",
    "text": "Experimentation at scale\nFacebook changed quickly. Temporal validity is a real problem when things change at this pace. But the problem was actually even worse than the sheer scale would suggest! Many of the biggest changes to how Facebook work were gated not by commits of code, but by A/B testing7. The number of active experiments at any one time was on the order of thousands. Even assuming each test had only two groups, this number implies that no two people were likely to experience the same Facebook8. I wouldn’t be surprised if, given the increase in employees relative to when we roughed out this number in around 2016, that number was an order of magnitude higher by 20219.\nIt’s worth thinking about how these experiments worked a little bit. One thing the handful of people implementing a single test absolutely could not do was to control everything about how the other 30k engineers run their tests. This is simply the reality of working on a project with this many other people.10 This led to difficulties occasionally, in which stuff about the platform changed and rendered a prior test uninformative or incorrect about the current state of the world. On my team, this is why we encouraged two practices: (1) long-run holdouts (where for important features, the rollout of the feature would be to ~99% of users rather than 100%) (2) rerunning important tests to verify results continued to hold.11\nEverything is evaluated relative to the sum total of other stuff Facebook is doing on the platform.\nThe reality of massive simultaneous experimentation and change resulted in internal experiments using the term “status quo” condition rather than “control” condition in experiments. Everything was evaluated relative to the sum total of other stuff Facebook was doing on the platform. As an example, check out the software my former team developed for doing complicated experimentation at platform scale. There is no “control” group, there’s just the “status quo”: what the platform is currently doing. The other groups are potential modifications of that behavior. The reason we evaluate relative to this group is because experimentation is a means of optimization12. Experimentation was a tool to improve how the system worked, so we wanted to evaluate our one change relative to the otherwise current state of the system. We never care about comparing to the state of the system 1 day or even 5 minutes ago.\nThis is one reason why I welcomed the eLetter by Bagchi et al at Science: it’s important to remember that “Facebook” (the platform) is a moving target13. In short, during the course of US2020, the company implemented a number of other measures aimed at combatting bad behavior during the election (“break-glass” measures). A lot of people forget how large and diverse an organization Facebook (the company) is. The “break-glass” measures discussed in the eLetter are a great example of this. One hand certainly can’t control what the other hand is doing, but it often isn’t even aware of what that hand is doing14. Unfortunately, I think this important message is undercut by another one in the eLetter:\n\nThis can lead to situations where social media companies could conceivably change their algorithms to improve their public image if they know they are being studied.\n\nThis implies a single-minded agency of the daily practice of Facebook engineers that is unreasonable (and largely impossible). As I tried to emphasize above, the company is huge, diverse and fast-moving15. The vision of a monolithic company exercising its will is simply inaccurate, especially when considering a project as small as US2020. Lots of things were changing during the course of this study; that’s (a) entirely normal at this scale and (b) why it’s important for us to be humble about its temporal validity (which I think we were)! The thing to be angry about here is not that the company tried to tweak the results of US2020 to make them come out hunky-dory: the thing to be mad about is that we have no truly reliable measures of the effects of these break-glass measures!16 We need more testing, not less.\nThe thing to be angry about here is not that the company tried to tweak the results of US2020 to make them come out hunky-dory: the thing to be mad about is that we have no truly reliable measures of the effects of these break-glass measures!"
  },
  {
    "objectID": "posts/what-was-us2020/index.html#was-us2020-independent-or-industrial-research",
    "href": "posts/what-was-us2020/index.html#was-us2020-independent-or-industrial-research",
    "title": "What was US2020?",
    "section": "Was US2020 independent or industrial research?",
    "text": "Was US2020 independent or industrial research?\nThis leads to what I think is a more important question about the nature of this project. Was US2020 industrial research, guided primarily by the needs of Facebook (the company)? The answer is clearly no. The academics proposed the designs, they had final control rights on the resulting papers and their salaries were in no way dependent on the results of these papers17. This is all critical, as there is an important difference between the Facebook and academic researchers in the project: Academia has academic freedom, while industry researchers do not. Does that mean that these papers are actually independent, then? Of course not. Seeking independence is a category error for this project. The internal researchers have incentives, as do the academics.\nFundamentally, these parties have different things they bring to the project: internal researchers knew how the platform works in detailed ways that no external person could and they had the capacity to actually work with internal platform data. They had been studying the ins-and-outs of this platform full-time for years. External researchers, on the other hand, were not burdened by corporate incentives and they had the academic freedom to speak freely about the project and their beliefs18. All of these perspectives are necessary for a successful collaboration, but it would be incorrect to suggest that they were equal partners. They cannot be equal, because they contribute in radically different ways.\nThe best one can hope for is to reconcile competing interests by writing down what everyone wants from the project, having an argument about which things can be agreed upon and then choosing to do those things or not. We already have a great tool for this kind of collaboration19."
  },
  {
    "objectID": "posts/what-was-us2020/index.html#pre-registration-as-a-contract",
    "href": "posts/what-was-us2020/index.html#pre-registration-as-a-contract",
    "title": "What was US2020?",
    "section": "Pre-registration as a contract",
    "text": "Pre-registration as a contract\nThe solution, I think, is setting out a clear contract between the parties at the start of the project. We actually already know what these are: pre-registrations. The pre-registration serves as a contract about exactly what you plan to study and how you plan to do that20. Filing it publicly in advance can then tie your hands and commit you to doing what you said. While problems inevitably arise post-data, it’s easy to file amendments as necessary. Cyrus Samii wrote about how he (and EGAP) sees pre-registrations that I think relates very nicely to this view, as he talks about the value of pre-registration as a conversation about the research you plan to do. It very much strengthens the quality of the work you will end up producing!21 When parties are collaborating on research and the parties have different incentives, a pre-registration serves as a Memorandum of Understanding. It gives you a specific document on which you can have an explicit negotiation about what you will do in a transparent way. For example, if the parties can’t agree on a definition of the ‘control’ group, then it must be redefined to reflect what can be agreed on. If in the course of executing the project, one side makes a unilateral decision about something substantively important that would not have been agreed to by the other side, then this is an error in the contract for lacking specificity over something important. Take the lesson and apply it to the next project.\nThis is the most critical part of the design of US2020. Take expert internal researchers who have a clear conflict of interest (but know more about the actual workings of the platform than anyone outside it) and combine that with independent researchers who are not beholden to the company and are only interested in learning something about how it works. Let these two sides negotiate on what would make a mutually beneficial research project, write it down, do that project22. The appropriate reading, then, of these studies is as the result of a dialogue23. And to the point of the eLetter which sparked this meditation, the pre-registration documents we put together make clear our choice of comparison group which is exactly why these documents are so valuable.24\nEarly on, it was decided that the project should speak with a single voice. This makes sense given how an academic paper typically works with only a couple authors: consensus works, and every author can speak their mind about the paper. In my opinion, it doesn’t work for a collaboration as large and diverse as US2020. In US2020, only the academics have the freedom to say what they think, and the design of the project means that they are always the ones who have the final determination over what gets written. In practice, this almost always resulted in decision by consensus. There was really very little disagreement because everyone sought the best scientific result. That said, disagreements existed. For example, from Michael Wagner’s report as the rapporteur of US2020:\n\nSpecifically, Meta researchers wanted to be able to expressly disagree with lead author interpretations of findings or other matters in articles they coauthored. Stroud and Tucker opposed this effort, noting that collaborators can remove their name from a paper if they have a fundamental disagreement.\n\nFrom the way the project was designed, this was the appropriate decision (downstream from choosing to speak with only a single voice), but for the design of future projects, I vehemently disagree with this structure. All members of US2020 are expert scientists, many of whom have different perspectives. The remediation for disagreeing with anything that appears in one of these papers is, for Meta researchers, to entirely take their name off of a project which they have poured, likely, hundreds of hours into: the nuclear option. Much better to have some forum (an appendix?) for authors to include a personal statement clarifying their personal perspective on the paper and how that diverges from the main text25. This acknowledges that no written document can perfectly express the views of dozens of scientists, and provides far more information about what the project shows and what may be debateable matters of interpretation. Anyone who has talked with a scientist knows that if you put two together in a room together you’ll have at least two perspectives on any topic under the sun. It also better expresses the reality that everyone involved in the project is a scientific expert: not merely those in academia. This sets up what I think is a common misapprehension about the project."
  },
  {
    "objectID": "posts/what-was-us2020/index.html#facebook-researchers-were-not-robots",
    "href": "posts/what-was-us2020/index.html#facebook-researchers-were-not-robots",
    "title": "What was US2020?",
    "section": "Facebook researchers were not robots",
    "text": "Facebook researchers were not robots\nIn the editorial published along with the eLetters, Holden Thorp and Valda Vinson wrote:\n\nIn a statement for this editorial, Chad Kiewiet de Jonge, a research director at the company, Meta, insisted (to H.H.T.) that it had been forthcoming about the emergency measures to the researchers.\n\nReading this, it might surprise you to learn that not only is Chad an author of the published paper in question, he’s in fact one of the lead authors. That is, he has been in all of these meetings where such things would be discussed! To quote from the detailed list of contributions:\n\nN.J.S. and J.A.T. were joint principal investigators for the academic involvement on this project, responsible for management and coordination. C.K.d.J., A.F., and W.M. led Meta’s involvement on this project and were responsible for management and coordination. [emphasis mine]\n\nFrankly, this has been a problem from the beginning. As I said immediately on publication, “This project was a deep collaboration, and generally, I think, not an adversarial one, either. Everyone wanted to get this right!” Nevertheless, it hasn’t always been presented like this. If you read the project as a dialogue between internal and external researchers (the accurate view, I believe), the only way that this could possibly be “independent” as in the standard set by Michael Wagner is if the Meta coauthors were robots who merely carried out the precise will of the academics26. This is clearly bad for the scientific output, and it isn’t what actually happened, as Wagner notes:\n\nOn occasion, this, and advice they received from Meta researchers, led the outside academics to reshape what it is they sought to study, or how they sought to study it, as they learned details about what data were collected and structured by Meta for analysis. For example, the outside academics were not precisely aware of how Facebook groups were joined and, in some cases, left, by users. Coming to understand this led to changes in the working paper examining behavioral polarization.\n\nAnd yet, there is an implication that Meta researchers were trying to hide things, as in the quote he shares from a former employee saying,\n\nFacebook researchers will answer every question they get from the professors honestly; they are ethical professionals. However, they are also corporate employees. If the professors don’t ask the exact right question, Facebook staff won’t volunteer that they know how to get what you (outside academics) are really asking for.\n\nIt’s very weird that this is attributed to a former employee rather than either the academics or to anyone involved in the project. From everything I saw, Meta researchers didn’t just narrowly answer direct questions, but bent over backward to try and re-interpret requests to get at what the academics’ questions intended. For my part, on one paper I spent a lot of time on, I wrote dozens of pages trying to redefine an unworkable analysis an external academic suggested to one that would be workable — while preserving the research questions. Other Meta employees did similar work on basically every single paper. That work is, in effect, an act of persuasion aimed getting the academics, who have the control over the content, to adopt the change or redefinition based on their view of the scientific merit. To what extent was it based on corporate incentives and to what extent was it based on scientific judgment? Likely a combination: I believe more of the latter than the former. But the negotiation of the pre-analysis plan is the way that this is adjudicated, and in US2020, the external academics were the only ones with the power to decide.\nI think the problem is that people can’t help but see Meta as a monolithic organization which exhibits a single will. This is seriously incorrect in almost obvious ways: employees disagree about, well, basically everything! Dan Davies’ book on the flows of accountability through the cybernetic lens is a useful correction to this vision. The company “Meta” is distinct from the sum of its individual parts: the procedures and processes that it uses to determine how to operate are largely not under the control of any singular person. In short, it is an organization and this imposes constraints on employees. And yet, journalists and editors of Science seem to have trouble understanding this. Internal researchers must seek permission to do this kind of research (a heavy constraint), but conditional on permission to do so, there is no instrument of Facebook’s will hanging over every discussion and decision they make. This is simply not how information is transmitted within the company27.\nSpecifically on US2020, employees fought for years to enable this kind of research. There was extreme skepticism among executives precisely due to a belief that there’s no way that the project could be “good” for Facebook. The project exists despite the fact that Facebook (the company) was not particularly in favor of it. Rather, the researchers you see listed as coauthors are on there specifically because they thought it was important to do rigorous, transparent social science about the effects of social media. As a personal anecdote, I was advised against participating in the project if I wanted to optimize my career at Facebook: it’s much more professionally rewarding to make the system work better than it is to do this kind of evaluative work28. That said, I don’t mean to paint all internal researchers as exactly like people who choose not to work for Facebook. The internal researchers are unlikely to have seriously negative beliefs about the platform, so they may demand stronger standards of evidence for harm than might be acceptable to people more willing to see the inherent evil of the company.29 That’s why you have the argument before you see the evidence: in a pre-analysis plan."
  },
  {
    "objectID": "posts/examining-the-apparatus/index.html",
    "href": "posts/examining-the-apparatus/index.html",
    "title": "Examining the Apparatus",
    "section": "",
    "text": "Kevin Munger has a great article out problematizing the idea of “The Algorithm”. You should read the Mother Jones article and then you should read his Substack1. They’re both great! And they touch on an important theme I’ll return to a lot, which is about the right ways to conceive of and examine complex systems.\nThe argument, in brief, is that “the Algorithm” is a misleading frame that elides our participation in the system. He proposes (based on the hottest philosopher of 2024, Vilém Flusser) “the Apparatus” as a better term that acknowledges each of our presence within this greater system which defines much of what we actually mean by “the Algorithm”.\nIn this blog, I’m going talk about the implications of re-formulating the source of our present weirdness as “the Apparatus”. How is the apparatus constructed? What are the nuts and bolts of how exactly we are implicated in its construction? Shockingly, I’ll argue we need more experiments."
  },
  {
    "objectID": "posts/examining-the-apparatus/index.html#the-apparatus-explained",
    "href": "posts/examining-the-apparatus/index.html#the-apparatus-explained",
    "title": "Examining the Apparatus",
    "section": "The Apparatus, explained",
    "text": "The Apparatus, explained\nI think it’s worth talking through how change happens in Tech. I’ll focus on Meta, because it’s the place I know best. This is basically the guts of how the Apparatus gets created.\nThe problem faced by Tech companies is, put simply “how should we decide what to do?”. There are a number of approaches to this, but I’m going to contrast two:\n\nBuild stuff and then ask people how/whether they like it.\nBuild stuff and then see how/whether people use it. We have to keep in mind, however, that we’re fundamentally talking about making decisions for a product used by (e.g.) 2 billion people. We have to think about how these two approaches would work in that setting, i.e. asking the question “Will it scale?”\n\nThe first approach would rely on large-scale surveys. There’s no real way to build up qualitative knowledge about 2 billion people, so we need to be quantitative, and surveys are pretty much the only game in town if you want to get folks’ opinions like this. There are hard limits here, though. At the limit, you can’t (for instance) ask every user how their experience of the site is every five minutes. At Facebook, the standard “cool-down” period after taking a survey was six months when I was there. This imposes hard limits on the precision with which you can actually measure the opinions of users2.\nThe second approach would rely on directly observing behavior and encoding this into meaningful measures about what people are doing. Most of this is just counting things that people are doing3. Because you don’t ask for any of your users’ attention to do this, there aren’t really any practical limits to your measurement of on-platform behaviors4.\nGiven these practicalities, it’s unsurprising that the result is an almost ideological belief in behavioralism: that the only way to understand what users want is to observe what they do5. My understanding is that this was part of an inspiring introductory speech that Chris Cox would give to new hires6."
  },
  {
    "objectID": "posts/examining-the-apparatus/index.html#the-apparatus-sustained",
    "href": "posts/examining-the-apparatus/index.html#the-apparatus-sustained",
    "title": "Examining the Apparatus",
    "section": "The Apparatus, sustained",
    "text": "The Apparatus, sustained\nThis process results in a cycle of iterative improvement. Matt Hindman’s book does a good job of describing the power of this cycle. It’s also the basis of the Agile software development philosophy, expressed at Facebook as “ship early, ship often” or, more provocatively, “move fast and break things”.\nThis saying has, I think, always been misinterpreted. The point is not that you’re okay with breaking things. The point is that you’re okay with trying new things, because you know that you have systems in place for identifying when things break. Many of the neurotic high-performers that get hired in tech are perfectionists, so you need to encourage them to not spend years working on something before launching it and seeing how it works. This should really be interpreted through a mantra of the Agile Manifesto:\n\nOur highest priority is to satisfy the customer through early and continuous delivery of valuable software.\n\nOver time, trying a bunch of stuff (some of which might seem crazy ex ante) and watching what users do in response to it is a great way to build a successful product. I think this is one of the defining features of the secret sauce of modern Big Tech.\nThis perspective flows into incentives throughout the company. Software engineers are, in large part, promoted and compensated based on how well their changes result in user behavior that is deemed “good”. The number went up? Congratulations, you get a bonus. You can break apart this monolith, but this isn’t a bad first order approximation7."
  },
  {
    "objectID": "posts/examining-the-apparatus/index.html#the-apparatus-examined",
    "href": "posts/examining-the-apparatus/index.html#the-apparatus-examined",
    "title": "Examining the Apparatus",
    "section": "The Apparatus, examined",
    "text": "The Apparatus, examined\nThis discussion should make it clear that there’s no sense in which any person understands “the Algorithm”, as Kevin argues. Wouldn’t that be comforting? Stafford Beer had a famous saying, “The purpose of a system is what it does”. This isn’t a reassuring sentiment. We have to work extremely hard to see and measure what a system does. There’s no easy shortcut through “intentions” to understand a system. Even with a truth-serum, we can’t just ask Zuck what he intended to build and then decide if that’s good or bad8. No engineer fully understands the entirety of the complex system they’re working on, so we can’t judge the worth of what they’ve built solely on what they intended to build.\nSo what is the purpose of the apparatus? Well, in one sense it’s essentially the aggregate of the incentives for all the people building the apparatus: making the numbers go up. If we want to understand the purpose of the apparatus, we need to encode the things it does into numbers, and measure how changes to the apparatus result in changes to those numbers. There’s only one consistent way to do that: experimentation9. And we need to find the right new numbers to measure, because there will undoubtedly be unintended consequences.\nBut we cannot forget that the essence of the behavioralism in Tech is that we are those numbers. We are as much a part of the apparatus as the engineers who build the system: every post, every click, every message, every linger over a story is part of the apparatus. This isn’t just true in the sense of our being complicit in the system. It’s true mechanically in the way engineers construct the system. The builders of the system want to give us what we want.\nThanks for reading Drew’s News! Subscribe for free to receive new posts and support my work."
  },
  {
    "objectID": "posts/whats-the-point-of-rcts/index.html",
    "href": "posts/whats-the-point-of-rcts/index.html",
    "title": "What’s the point of RCTs?",
    "section": "",
    "text": "Kevin Munger and I recently had the joy to write a response to “Nonparametric Identification is not enough, but randomized controlled trials are” by the all-star team of Aronow, Robins, Saarinen, Sävje and Sekhon (ARSSS). You can tell Kevin and I like this paper because as of this moment, we make up about one quarter of its total citations on Google Scholar. I believe a number of other responses will be coming out soon; I’m very excited to read them.\nI’m going to riff a bit more here about our response, “Enough?”. In particular, I want to dig into what I see as the most important perspective we articulate in the paper and some of its further implications. Namely, we argue that the biggest value of experiments is ontological1."
  },
  {
    "objectID": "posts/whats-the-point-of-rcts/index.html#enough-for-what",
    "href": "posts/whats-the-point-of-rcts/index.html#enough-for-what",
    "title": "What’s the point of RCTs?",
    "section": "Enough for what?",
    "text": "Enough for what?\nWe started off by trying to think through what it would, conceivably, take for an estimate to be generalizable knowledge. We identified 4 things that we certainly want to be able to generalize over: (1) sample, (2) site, (3) realization of a theory, and (4) time. The first two are fairly well-trodden. Convenience samples may make it hard to generalize to populations of interest, and site-selection bias may make it difficult to generalize to the locations we’d like to know for future policy choices. Tal Yarkoni has already talked through the multiple challenges of generalizations across various dimensions in the language of random effects in Psychology.\nThe third element is kind of weird. What exactly is the set of treatments we could have defined based on a particular theory we’re trying to test? I have a bit of a pre-occupation with this question because it was my experience running message-tests at Facebook that while psychological mechanisms may be important, so are the specifics of wording that are largely unrelated to the mechanisms. If you write text that sounds like a robot, it’s gonna suck even if you’re pulling on the right mechanism. If we want to generalize the results to the theory being good or bad, then you have to be able to abstract away from this.\nThe final point, on time, is a long-time hobby horse of Kevin’s. One of the main criticisms of this focus is that it isn’t actually any different than other forms of generalizability. We identify one extremely important way that time is different: there’s no ‘shoe-leather’ based solution for it. You can random sample from a population, a set of sites or from potential treatment definitions. You cannot randomly sample time. This is an extraordinarily important difference, because it means, ultimately, that you must rely on modeling assumptions that may not be necessary in these other cases (for exactly the reasons ARSSS spell out: randomization is an extremely powerful tool to eliminate such assumptions).\nI strongly believe in the power of shoe-leather3. The fact that it is insufficient for temporal generalization should be concerning4. Regardless, our point is just that randomization is good, sure, but it doesn’t get us to where we want to go scientifically. What does makes it good?"
  },
  {
    "objectID": "posts/whats-the-point-of-rcts/index.html#control-of-what",
    "href": "posts/whats-the-point-of-rcts/index.html#control-of-what",
    "title": "What’s the point of RCTs?",
    "section": "Control of what?",
    "text": "Control of what?\nI think the best part of the paper focuses on the ontological value of experimentation. That is, experiments create novel states of the world, and this is a powerful way to imagine alternatives to the social reality we currently inhabit. It creates incentives for people to create cooler and more ambitious changes to the world while measuring what those changes do. But I think this gets at another reason that RCTs are particularly useful.\nObservational methods assume a complicated ontology in the course of creating a DAG: how to chunk up the social world into nodes and how to assign values for those nodes to each unit. Put simply, the social world is extremely complex, and this process is subject to extraordinary error. RCTs circumvent this difficulty entirely: their accuracy does not depend on such ontological assumptions. Instead, they impose their ontology. In the paper, we focused on the ontology of the treatment, but the argument is much broader and deeper.\nFor example, suppose that we run some field experiment that changes substantial aspects of people’s media diet and we measure effects on various constructs that we think are interesting: ideology, perhaps, beliefs about facts, etc. It isn’t just that the RCT imposes the ontology of treatment and control, the RCT provides information on all of these constructs that we create (i.e. causal effects on these constructs). In survey research a common recommendation is to avoid regressing one survey construct on another5. But the truth is, we do this with all observational research, as we make assumptions about constructs everywhere. With the RCTs, we only do this on one side of the equation. The treatment ontology is imposed, but this allows us to weaken the assumptions of ontology in the outcomes. We will still have valid causal effects on whatever ontology we choose.\nOr maybe in our field experiment, we measure something collected administratively about behavior (e.g. turnout in an election). In this case, both the treatment and the outcome have some ontological precision in their meaning6. What about the heterogeneity we may observe? Well, we don’t rely on the ontology the same way we do in observational settings: to take an ontology that is currently the subject of political contention, consider measuring heterogeneity by gender identity. There are different ways to construct this ontology, but when we choose such a construction, we can simply measure the heterogeneity according to this ontology. If we’re instead in an observational world, we would need to care much more about getting this ontology “correct” in some way as it pertains to selection into treatment7. In an RCT, we do not rely on this correctness. We can explore variation with respect to whatever ontology we choose and, perhaps, even make judgments about which ontology better reflects the variation in response.\nRCTs give us a solid foundation from which to explore these questions, while observational work cannot do this so easily."
  },
  {
    "objectID": "posts/whats-the-point-of-rcts/index.html#for-whom",
    "href": "posts/whats-the-point-of-rcts/index.html#for-whom",
    "title": "What’s the point of RCTs?",
    "section": "For whom?",
    "text": "For whom?\nCritically, however, the ontological power of experiments is premised on control of the world. It is power not just in the statistical sense of charts and numbers, but in the sense of guns and laws. Wielding this form of power requires responsibility and humility.\nExperimentation can be a tool for achieving our country. By trying out new social possibilities, we can provide the foundation on which a better society can be built. Doing this requires venturing past is and into ought. This is fraught territory for scientists, so must be part of a larger democratic process. As the fabric of society is increasingly constructed of bits and code, we can increasingly implement creative new interventions that are meaningful to people’s lives. I think, in fact, we’re ethically obligatedto do so.\nBy making our focus broader in this framing, we do something that ARSSS could not: draw distinctions between types of experiments and demonstrate the value of experimentation within a larger experimenting society. All experiments enjoy the statistical benefits that ARSSS describe. Not all experiments are similarly powerful ontological tools for considering new social worlds. The experiments that measure up best under this perspective are the ones that attempt ambitious changes to social reality. The experiments that do poorly are, mostly, what we have: survey experiments. These are, surely, not enough.\nRead the paper and let us know what you think!"
  },
  {
    "objectID": "posts/is-bluesky-convivial/index.html",
    "href": "posts/is-bluesky-convivial/index.html",
    "title": "Is Bluesky convivial?",
    "section": "",
    "text": "Back in November of 2023, I was lucky enough to participate in a “Morality in Tech” workshop hosted by Princeton’s CITP put on by Kevin Munger. It was a fascinating interdisciplinary group of people trying to wrap our heads around how to grapple with how to tie moral and ethical frameworks to technological change. It culminated in the “Building the Society We Want” syllabus.\nThis post is a modified version of a talk I gave on the historical development of Smalltalk, and how I see it as a fundamentally different way of thinking about technology than we’re used to today. Unfortunately, I ultimately don’t see it as a way out of the mess we’re in on the social web. Bluesky cannot resolve the fundamental problems with microblogging, which are fundamental to the technology."
  },
  {
    "objectID": "posts/is-bluesky-convivial/index.html#what-is-smalltalk",
    "href": "posts/is-bluesky-convivial/index.html#what-is-smalltalk",
    "title": "Is Bluesky convivial?",
    "section": "What is Smalltalk?",
    "text": "What is Smalltalk?\nSmalltalk was developed by Alan Kay, Dan Ingalls and Adele Goldberg at Xerox PARC in the 1970s. What it was is a little hard to frame in terms of our contemporary categories around computing. It was more than a programming language and not exactly an operating system as we’ve come to understand them. It was the software part of Alan Kay’s overarching personal computing vision (from back before computing was really personal computing). Smalltalk had a lot of things that are now very common, but were very uncommon when it came out. It was an early iteration of Object-oriented programming1, it had an early graphical interface, all code was just-in-time compiled (and could be recompiled on the fly), and it integrated a development environment into how the user interacted with the system. But Smalltalk was also a philosophy of computing:\n\nlook for the distinction between a programming language and a programming system, and consider the difference in providing a system in which the user can feel individual mastery over complexity\n\n(emphasis mine) The intention was to make the vast complexity of a computer system something that end-users could control. The system should allow the complexity of the system to be comprehensible to the user, and allow them to adapt and reorient it at their will.\nA lot of what I’m drawing on here is from the 1981 issue of BYTE Magazine in which PARC chose to write a public introduction to the newest iteration of the Smalltalk system. It’s available online and a truly fascinating read that I can’t recommend enough.\n\n\n\nImage\n\n\nSmalltalk was intended to give the User the power of God over the computing system. This isn’t really subtext. In this introduction to Smalltalk, they provide the following illustration:\n\n\n\nImage\n\n\nOn the far left, the System Programmer (God) creates a system (the Taj Mahal). The Programmer creates things (a bridge) for the end-user using the things provided by the System Programmer. Smalltalk was intended to democratize the power of creation, moving into the second figure from the left, in which the Programmer creates kits that the User can use to construct what they want. In Figure 3 this goes even further, as Users can create the kits themselves that they can assemble into what they want. On the far right, however, the idea is that Programmers / Users should be able to climb up above God and impose their own will over that of the System Programmer: to become God.\nIt’s a vision of a radically empowered user who can manipulate every aspect of their computing environment. In my mind, this vision is exactly what it means to have a “convivial” tool in the sense of Ivan Illich: tools which empower human liberty rather than constrain.\nThere are costs, however, as Smalltalk asks much more of users, as Kay and Goldberg write in 1977:\n\nThe burden of system design and specification is transferred to the user. This approach will only work if we […] allow ordinary users to casually and easily describe their desires for a specific tool.\n\nBut the benefit is a fully “reactive” system2. What did this look like?\n\n\n\nImage\n\n\nEssentially, you had a browser like the above, through which you could navigate through your system, then the panel at the bottom allowed you to edit the code defining how that part of your system worked. You could then choose to re-compile that part of your system and it would automagically update to follow your new instructions. Voila, complete control over the system, manipulable at runtime.\nBut there’s one last quote, from Daniel Ingvalls from his article about the design principles of Smalltalk:\n\nNatural Selection: Languages and systems that are of sound design will persist, to be supplanted only by better ones.\n\nOur computing is not based around Smalltalk these days, nor does it look very much like this: it died out in favor of opaque systems with a lack of user-control. It seems that users were, in the end, not willing to take the burden of full responsibility for the functioning of their own computing environment."
  },
  {
    "objectID": "posts/is-bluesky-convivial/index.html#empowerment-is-still-about-power",
    "href": "posts/is-bluesky-convivial/index.html#empowerment-is-still-about-power",
    "title": "Is Bluesky convivial?",
    "section": "Empowerment is still about power",
    "text": "Empowerment is still about power\nI think the question of inevitability is what I really struggle with here. The dream of a system that radically empowers the user is beautiful. But are users willing to actually take this power? Can they? In grappling with this question, I tried to think about affordances of social media and the extent to which they align with Smalltalky principles.\n\nBluesky lets users choose their own Feed algorithm.\nThe hashtag was created as a tool for users to take control of discovery and categorization themselves.\nDistributed social media services (e.g. Mastodon) seek to allow individuals to choose the community (and, therefore, community norms) they wish to belong to.\nindividual choice about network links to create (e.g. friending on Facebook): this is the source of a lot of ideological polarization, rather than something done “to” users by algorithms. In each of these, the user gets pretty profound ability to shape what their social media is. But it’s still not enough! My choice of a feed algorithm doesn’t give me the experience of using Bluesky that I would have if everyone were to use my preferred feed. In order for me to “choose” my experience on the platform, I need the power to choose how everyone experiences the platform. Empowering users doesn’t avoid the critical question of who holds the power to make such changes.\n\nIn a previous post, I claimed that there were four main definitional characteristics of microblogging: (1) short texts (2) virality (3) simple engagement possibilities and (4) an infinite feed. The users can’t alter these characteristics on BlueSky or on any other platform. Removing reshares from your own screen doesn’t change whether you experience virality and the associated credibility cascades and everything else associated with virality. To kill virality, you’d need to make aggressive changes like removing the visibility of likes from everyone’s screen, removing ranking based on likes, views, clickthroughs, killing reshares globally and the list of changes goes on. After making all of those changes for everyone then maybe you’ve done a good job of breaking virality. Can you give virality back to a single user, at that point?\nIt is fundamentally impossible for everyone to alter everything about their social media experience to their whims, while continuing to make it meaningfully social. Even in a world where everyone gets to choose everything about how they interact with social media, they don’t get to choose how everyone else interacts with social media3. Our experiences online aren’t just shaped by our own choices, but by the choices of those around us4. We can’t “solve” our experience by changing only our own affordances.5 Bluesky can only ever give the illusion of autonomy, it can never provide true control over the environment as envisioned by Smalltalk. That autonomy only works for a computing platform that is fundamentally a-social: it interacts only with you and responds only to your preferences.\nConvivial, anarchist empowerment to shape one’s own technical reality doesn’t offer a way out of the shared reality we create with technologies. At best, it can offer a patchwork of small fixes. We cannot fundamentally alter the medium, because the medium is defined by what is shared. The critical question is who has the power to choose the shape of the overall technology, and who gets to study how the world is changed by those choices. Right now, only the platforms have these powers."
  },
  {
    "objectID": "posts/whats-the-point-of-rcts/index.html#footnotes",
    "href": "posts/whats-the-point-of-rcts/index.html#footnotes",
    "title": "What’s the point of RCTs?",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nYou may have read some about this perspective on Kevin’s blog.↩︎\nChris Harshaw has a response to ARSSS that goes more in this direction focused on the importance of the design-based frame of thought (and its rhetorical power).↩︎\nIf you’re reading this blog, I hope you’re already familiar with the beautiful David Freedman paper on this subject, Statistical Models and Shoe Leather. If you aren’t, read it immediately.↩︎\nShoutout to the GOAT.↩︎\nI’ve done a little searching, but I’d be interested in some citation archeology / intellectual history. Where does this recommendation originate?↩︎\nOr at least something like phenomenological precision.↩︎\nIf you’ve ever read into the literature on measurement error of covariates for observational causal inference (its a nearly intractable problem), this should make you sweat.↩︎"
  },
  {
    "objectID": "posts/plagiarism-is-bad/index.html#footnotes",
    "href": "posts/plagiarism-is-bad/index.html#footnotes",
    "title": "Plagiarism is bad",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nYes, this is also surprising to me.↩︎\nI’m very consciously not talking about GPT here, but, like, also that.↩︎\nA single number with 4 decimal precision is equal to log₂(10^4) = 13.3 bits, assume there are a couple of estimates.↩︎\nCitation also lets us bypass the hard limits of page limits, too — citations let us gesture at much more complicated ideas than we would be able to fully explain in our particular pdf.↩︎\nEven in this case, we are different people than the authors of the replicated work and probably have different opinions about how it relates to the world. At the very least, time has passed and the literature is in a different place!↩︎\nHarder to do this on a dissertation which may not be able to be coauthored. But honestly, to a first approximation I’ve only cared about one person’s dissertation (Ferenc Huszár’s).↩︎\nI’m taking this example as a jumping off point, I don’t think it’s like, “academic misconduct” to use boilerplate language, and I think his example is a very good one, because tons of people have copied that kind of language.↩︎"
  },
  {
    "objectID": "posts/is-bluesky-convivial/index.html#footnotes",
    "href": "posts/is-bluesky-convivial/index.html#footnotes",
    "title": "Is Bluesky convivial?",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIn fact, Alan Kay doesn’t actually think modern OOP is at all related to what “OOP” was for Smalltalk. If I had to condense down what I understand he sees as the critical distinction, I’d say that it would be something about autonomy: his vision saw individual “objects” being more-or-less autonomous computing units, which would communicate with one another and this would allow the programmer to create complex systems based on these interactions. Crucially, he saw this as a lot more than a paradigm for writing and organizing a particular piece of software.↩︎\n“The Smalltalk programming environment is reactive: the user tells it what to do and it reacts, instead of the other way around.” Tesler 1981↩︎\nAnd, in particular, a world in which collaborative filtering continues to play a strong role in what content your network (or other people more broadly) sees.↩︎\nThe most banal and obvious platitude of social science: “other people matter for your experience of life”↩︎\nWell, you can just sign off completely. And maybe you should! But that’s a different question altogether. For what it’s worth, I’ve basically signed off. I don’t think the bargain is a positive one in my life.↩︎"
  },
  {
    "objectID": "posts/calibration-as-an-hte-diagnostic/index.html#footnotes",
    "href": "posts/calibration-as-an-hte-diagnostic/index.html#footnotes",
    "title": "Calibration as an HTE diagnostic",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThere are some reasons to expect that in an RCT a T-learner shouldn’t be too bad: you don’t need to correct for propensity scores (they’re all the same), so you really just want a model to help you extrapolate the tiny amount from nearby factuals to counterfactuals. Soren’s paper on X-learner has some simulations that bear this out. I want to be very clear that I am not recommending that you just stick with T-learners in RCTs, though! I’m only making the very weak argument that it isn’t crazy to think they might do a decent job as a first-cut.↩︎\nA lot of very good computer scientists completely balked at the question, though. It requires actually thinking about the specifics of the application rather than just looking at an AUC number and making an evaluation.↩︎\nAn alternative approach that may be even better is to use the Chernozhukov et al. procedure. We don’t directly compare to this, largely because our motivation starts from the position of diagnostics rather than estimation.↩︎\nThis was definitely reasonable from their perspective: there’s no sense in which the procedure we’re proposing is a panacea to the problems of HTE estimation (I’m increasingly negative on whether there is such a “correct” procedure).↩︎\nThe GRF package has something they refer to as a “test” of calibration, but I think it’s an important distinction to provide these kinds of tests in a visual way. You will almost certainly turn up more problems when you are looking at more than just a single number (e.g. criticisms of the NHST paradigm compressing all information into a single p-value).↩︎\nBasically, what you get is the following, using the fact that ridge regression is basically just a linear rescaling of OLS (assuming the covariates form an orthonormal basis). The coefficient vector, β, is the set of “true” linear coefficients of the best linear predictor to the CATE function, λ is the regularization parameter:7Technically we’re minimizing a bound on MSE (albeit a tight bound). We have a whole section (4.3 and 4.4) problematizing the idea of optimization here, because ultimately you just can’t know enough about your data a priori to make “optimization” something that fully makes sense. In particular, I like the counterexample we define in Section 4.4 which I think makes the dependence on unknown properties (the conditional variance) very clear.↩︎\nFootnote 7↩︎\nThe assignment process is basically not randomized, except up to a relabelling (i.e. swapping treatment and control). There are randomized extensions to this that would probably work quite well and efficiently, but which we haven’t spent the time to put together — feel free to reach out if you want to talk about them.↩︎"
  },
  {
    "objectID": "posts/stop-looking-for-the-next-twitter/index.html#footnotes",
    "href": "posts/stop-looking-for-the-next-twitter/index.html#footnotes",
    "title": "Stop looking for the next Twitter",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nNote: this is bad. My hot take is that thinking is good and we should try to do more of it and improve how we do it. The simple binary “engagement” actions constrain human behavior into radically low dimensional signals. This is basically what Kevin means when he talks about online platforms making us behave like machines. This is not good! There are no posts for which my reaction can truly be summed up by a binary “Like” action! A RT sometimes is an endorsement, but sometimes it isn’t! None of this is expressible in the platforms!↩︎\nBy privacy, I mean that the ability to pretty strongly control how wide the distribution of your content is (such as by sending something in a group chat where only your wife and brother-in-law can read it). Encryption, cybersecurity and data protection are important components of privacy, but not what I’m talking about.↩︎\nAn analogy I like is that threads are in the style of a poetry slam (derogatory). It’s not inherently “bad”, but it is a very distinct style which focuses on having meaningful “beats” every couple dozen words or so. Good tweets in a thread often are basically glorified figures (with caption). That’s a very constraining style!↩︎\nComments were essentially “the problem seems important and there’s a lot of math which seems correct”. Some reviewers even called it easy to read, which we all had a good laugh over. The math was abstruse and we literally introduced a parameter without ever explaining why it was important!↩︎\nIt also hasn’t always been this way. A good example is Norbert Wiener’s Cybernetics, which is a ludicrous book in a lot of ways, but it was also not targeted solely at an in-group of very technical readers. Only some chapters are.↩︎\nI think an Oped length of about 1000 words is a good one to shoot for (don’t check how long this post is, sorry), e.g. Alex’s nice paper.↩︎\nControl and privacy seem to cut against the democratizing impulse, but they don’t have to. You can, for example, be gracious in admitting people and harsh in throwing them out when they don’t abide by community norms. The key democratizing element isn’t in making everything public, but in giving people a chance. Being public can actually be much worse in some cases: Imagine saying something kind of dumb and going viral as a grad student (when we are all very silly). Not good!↩︎\nI don’t even know what “work” means here. Just think about this community formation as expanding the groupchats you’re already having good discussions in. Is it a “failed” groupchat if you stop talking in it at some point? Obviously not! Is a coffee meeting a failure if it doesn’t result in a permanent collaboration? Spin something up and see how it goes!↩︎"
  },
  {
    "objectID": "posts/examining-the-apparatus/index.html#footnotes",
    "href": "posts/examining-the-apparatus/index.html#footnotes",
    "title": "Examining the Apparatus",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThis editorial line of this blog is generally pro-Munger.↩︎\nIt would be interesting to write down, under reasonable assumptions, exactly what this would impose as the minimal detectable effect. It would probably be small (for social science), but implausibly large (for the practical decisions about product made 1000 times a day in tech).↩︎\nThere was a joke at Facebook that this was the main skill of data scientists: we invented a whole job just to do advanced counting. Move fast and count things!↩︎\nTechnical limits exist. Tracking and storing all mouse movement for all 2 billion users would be an absurdly large amount of data collection. More commonly, the practice would be to radically downsample any of this kind of data that is collected.↩︎\nOf course, there’s also a “revealed preferences” element to this, but even if you ignore that, there’s still a practical matter of scaling.↩︎\nSomehow, I didn’t get this either as an intern or when I started as a full-time. This is sad, as I’ve heard is was a fascinating romp through the history of technology in a very McLuhan way. Oh well.↩︎\nI think you can break a lot of work in Tech into three groups (excluding support staff that doesn’t work directly on product): engineers (make numbers go up), UX researchers (make sure we choose the right numbers to go up), internal measurement tool builders (make sure when we say number goes up, it actually went up).↩︎\nI think many critics would be surprised at the prevalence good intentions here.↩︎\nThere’s an argument here that this is granting too much to behavioralism: that framing the problem in these terms is already giving up the game. I don’t think this is true. A strong argument for behavioralism is that it grants a common epistemological playing field that leads to productive arguments about what should be measured and what should be valued: a regulatory framework. Those are much more productive, I think, than arguing about more basic epistemological points on which there can never be agreement.↩︎"
  },
  {
    "objectID": "posts/what-was-us2020/index.html#footnotes",
    "href": "posts/what-was-us2020/index.html#footnotes",
    "title": "What was US2020?",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nKevin and I were noodling on what kind of commitment US2020 was on the scale of Meta, and he came up with the following: “Millions of dollars and thousands of hours by highly-paid employees — let’s go nuts and say it cost them $25 million. This is compared against $134 billion in revenue for 2023. That’s .019% of their annual revenue devoted to this 6-year project.” [source] This is a huge commitment to social science research, but this is simply not a large scale project by Facebook standards. Reality Labs burns like $15 billion per year.In contrast, financial institutions commit around 15% of their employees to compliance.↩︎\nSee Kevin’s post on this, I think it’s a critical standpoint for understanding modern scientific production.↩︎\nI can’t find a definitive answer online, but a Quora answer quotes 58% and Gergely Orosz cites 43%. 50% seems like it roughly jibes with my intuition.↩︎\nThe number of major SEVs (read as “major bad thing”) caused by a commit reviewed by a colleague with only the four letters “LGTM” as the review is… a lot.Note: if you do this in “someone else’s code” they are liable to get extremely mad at you.↩︎\nA lot of the blame for this falls on bots that do “automatic” commits.↩︎\nInfamous? Certainly had some of the best war stories of deployment at early Facebook I received in bootcamp.↩︎\nThis also led to plenty of bugs that only popped up in particular combinations of features gated by A/B tests, code, and a variety of other technical means.↩︎\n(2 treatments per test) ^ (number of independent tests) is much larger than 8 billion for any reasonable number of independent tests. There are a lot of independent tests at Facebook due to the “universe” concept (the same basic idea described by Google here).↩︎\nIf A/B tests scale linearly with employees. There were 17k employees in 2016 and 72k in 2021, an increase of 4x.↩︎\nThere are, of course, some exceptions: global holdouts that all employees have to respect, or particular features which interfere with each other strongly. These are both handled with technical solutions. The former by allowing global restrictions on who can be put in experiments and the latter through the use of “universes” (for more, see the paper on PlanOut, for instance) that ensure that users cannot be put in multiple conflicting tests.↩︎\nIf this post wasn’t already way too long, I would provide some examples. One notable one was when, between iterations of text optimization, the translation systems changed, vastly affecting our results.↩︎\nThis perspective, more than maybe any other, was key to what our team was good at. Figuring out what metric to optimize (metric definition) and what levers to pull were often (treatment definition) more essential than the ML toolkit we used for actually doing the optimization, which was fancy and useful, but also ultimately subservient to the problem definition.↩︎\nThey frame this as an issue of “validity”. In our response, we note that they are only referring to external validity, but I find the term misleading in general as Kevin and I discuss in our working paper on partial identification and temporal validity: This word has the unfortunate implication of being binary; computer login passwords and driver’s licenses are either valid or invalid. To say that a driver’s license is “mostly valid” is to say that it is “not valid.” Scientific knowledge is not binary, and while most practitioners can successfully keep this reality in mind when discussing “external validity,” the term introduces unnecessary confusion.And as far as temporal validity goes, as I quoted in my first post on this newsletter:Right now, about 15% of content in a person’s Facebook feed and a little more than that of their Instagram feed is recommended by our AI from people, groups, or accounts that you don’t follow. We expect these numbers to more than double by the end of next year.The current News Feed on Facebook is simply not the same thing we evaluated: the company moves too fast for that. If the only research we deem valid is research which is perfectly temporally valid 4 years on from its setting, we will have no evidence-base on which to make policy at all. This is an important problem we must contend with as a question of meta-science.↩︎\nIt’s hard for me to remember, but despite being an employee and working on US2020 I think I only heard about these break-glass measures through public reporting, rather than from internal communications, but I generally worked on methods stuff rather than specifically on applied civic stuff.↩︎\nIt is not without reason that employees have been described as Chaos Monkeys.↩︎\nAs far as I know, these measures weren’t even implemented in a way to test things internally. but I could be wrong about that, I have no special vision into this.↩︎\nI suppose you could say that the results would change the amount of prestige afforded to them. If this is the case, would the largely null results we found have been the best for their careers?↩︎\nUnlike other participants in US2020 from the Meta side, I actually now have the benefit to speak freely, so I’ve got that going for me.↩︎\nI’m specifically not referring to this as adversarial collaboration, because I don’t think that’s an accurate reading of the reality. It really wasn’t that adversarial, because the internal researchers also wanted to know the answers to these questions. We didn’t collect information on things like race or politics as a matter of course.↩︎\nNote that my argument here is completely divorced from the statistical arguments in favor of pre-analysis plans.↩︎\nNote that this justification isn’t a narrow one about getting a hypothesis test to cover at an appropriate rate, it’s about actual substantive concerns of what to study and how to study it.↩︎\nOf course, no plan survives contact with the enemy and every data scientist knows that the only true enemy is data.↩︎\nA dialectic?↩︎\nSome selections from the pre-registrations: The News Feed of users who are assigned to the control condition will not change as a result of the experiment.from the Likeminded exposure PAPUsers assigned to the control condition will see their normal News Feed under existing Facebook policies and procedures.from an as-yet unpublished study’s PAPUsers who have consented are randomized into three treatment conditions (or a control condition shared with other studies).from the Chronofeed PAP (emphasis mine)The second statement, I think, makes this the most clear. Facebook has existing policies and procedures, and the scope of this study will not be changing that. Given the size of Facebook (which I have repeatedly emphasized), I don’t think this should be surprising!↩︎\nSince the papers were not reviewed by Meta for content, this would give internal researchers some very small measure of the academic freedom that they do not, in general, enjoy.↩︎\nAcademics, mind you, who simply don’t know how internal systems, logging and metrics actually work. There’s no reason they could know this, in fact.↩︎\nTo fully think through the question would require a discussion about how performance is judged at Meta. I think the best way to think about this is that Facebook uses a form of stack ranking, where your manager has about 20 seconds to advocate on your behalf before putting you in a ranked list of your colleagues in the same organization and at the same level. The stuff that will be most persuasive are large impacts of your work on important corporate KPIs.↩︎\nBecause you can measure the value of making a system work better: you run an A/B test and see what your change does.↩︎\nThis is another argument for RCTs, as I think this debate pops up much more clearly in observational causal inference where you have to determine your willingness to balance the potential for bias against the desire to get some answer to a thorny question no matter how flawed. To be explicit, I suspect internal researchers are much less willing to accept any bias, partially due to a strong internal culture in favor of RCTs. Thomas Leavitt has a related argument on this point about the epistemic value of randomization.↩︎\nLet’s leave a holdout of 0.5% of the Facebook population from receiving these measures next election to find out!↩︎"
  }
]