{
  "hash": "fe8b9c4048d86e48c4e12ef10a620310",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Website Refresh\"\ndescription: |\n\ndate: \"2025-10-27\"\ncategories:\n  - technology\n  - website\nimage: main-image.jpg\ndraft: true\n---\n\n\nI've made a few changes and updates to this website. I think a number of the changes will be interesting and useful to others, so I'm going to introduce them briefly and discuss how they work.\n\n1. Semantic Scholar integration for updating publication records\n2. Listmonk tools for sharing blog posts as a newsletter\n3. Miscellaneous improvements to the design and performance\n\nI think these changes are consistent with [the philosophy I introduced in the last post](posts/back-to-basics/).\n\n## Semantic Scholar integration\n\nI've had a consistent desire to avoid tedious data entry in updating my website. This was part of my intention when I [moved it to Quarto in the first place](posts/quarto-website): Quarto gives great opportunity to programmatically generate a static website. I can easily mix in rich documents that combine code (R and Python in particular) and easily formatted Markdown text. This is all great. It let me hack together a nice way to display my publications in a consistent format based on data stored in a hand-curated YAML file. The next step was to automate the process of updating the publication records. I used the [Semantic Scholar API](https://api.semanticscholar.org/api-docs/) to fetch the latest information about my publications and then used [Quarto's templating capabilities](https://quarto.org/docs/websites/) to generate the updated publication list.\n\nMost of the work of this system is in two functions which, because I vibe-coded it, I've never really even looked at.\n\n<details><summary>See the code</summary>\n\n::: {#9581d701 .cell execution_count=1}\n``` {.python .cell-code}\ndef fetch_author_papers(author_id):\n    \"\"\"Fetch papers from Semantic Scholar API\"\"\"\n    base_url = \"https://api.semanticscholar.org/graph/v1\"\n    fields = \"paperId,title,authors,year,venue,publicationDate,externalIds,openAccessPdf,url\"\n\n    url = f\"{base_url}/author/{author_id}/papers\"\n    params = {'fields': fields, 'limit': 1000}\n\n    try:\n        response = requests.get(url, params=params)\n        response.raise_for_status()\n        return response.json()\n    except requests.RequestException as e:\n        print(f\"Error fetching data from Semantic Scholar: {e}\")\n        return None\n\ndef create_yaml_entry_from_ss_paper(paper):\n    \"\"\"Convert Semantic Scholar paper to YAML entry format\"\"\"\n    paper_id = paper.get('paperId')\n    title = paper.get('title', 'Untitled')\n    year = paper.get('year')\n    venue = paper.get('venue', '')\n\n    # Process authors\n    authors = []\n    for author in paper.get('authors', []):\n        author_name = author.get('name', '')\n        if 'Dimmery' in author_name:\n            authors.append(\"me\")\n        else:\n            authors.append(author_name)\n\n    entry = {\n        'title': title,\n        'authors': authors,\n        'year': year,\n        'venue': venue if venue else None,\n        'visible': False,  # Default to not visible\n        'ssid': paper_id,\n    }\n\n    # Add external links if available\n    external_ids = paper.get('externalIds', {})\n    if external_ids.get('DOI'):\n        entry['published_url'] = f\"https://doi.org/{external_ids['DOI']}\"\n\n    if external_ids.get('ArXiv'):\n        entry['preprint'] = f\"https://arxiv.org/abs/{external_ids['ArXiv']}\"\n\n    # Add open access PDF if available\n    open_access = paper.get('openAccessPdf')\n    if open_access and open_access.get('url'):\n        entry['pdf_url'] = open_access['url']\n\n    return entry\n```\n:::\n\n\n</details>\n\nCritically, this is all pretty trivial, but it's also *boring*, so I'm glad I didn't need to think particularly hard about it. It also means I didn't need to personally dig through the Semantic Scholar API documentation. The code quality is not great, but neither was my own code before!\n\nSo now when I build the site, it automatically updates the YAML with my latest papers. I can just browse through them add flip the `hidden` flag to show them once I'm satisfied they're right.\n\n## Listmonk tools\n\nThe second main improvement I made was to add an email distribution list to my website's blog. This was pretty easy because of the great open source email distribution software, [Listmonk](https://listmonk.app/), and the super easy deployment option at [Pikapods](https://www.pikapods.com/). The system I've come up with is basically the following:\n\n1. Write a blog post in a Quarto document.\n2. Push it to a branch if I want to have sensitivity readers.\n3. Once I'm satisfied, push it to main, which automatically triggers a Quarto build of the website and makes the blog post \"live\" on the website (and RSS feed)\n4. **Manually trigger a Github Actions workflow to schedule a Listmonk email campaign.** \n\nIt's this last part that I think is particularly cool. Basically the way it works is I vibecoded an over-engineered system that pulls the necessary metadata from a given blog post (in the `.qmd` file), renders the post to an HTML version, extracts the HTML from that, and sends all of this through Listmonk's API to schedule an email campaign with nicely formatted HTML content. I'm not going to embed all of the scripts for this here, but you can take a look at them in my website's Github: [`scripts/`]().\n\nTo actually trigger this monstrosity, I used a fun function that Github Actions provides: `workflow_dispatch` event triggers. Basically, what this means is that [you can configure a Github Action to trigger manually through the Github UI](https://docs.github.com/en/actions/how-tos/manage-workflow-runs/manually-run-a-workflow). It looks something like this:\n\n![Workflow Dispatch example](workflow_dispatch.png)\n\nSo basically all I need to do is tell the workflow what post to schedule (the `post_slug` part), when to schedule it (the `send_at`). There are also a few optional things I can plug in, such as which list to send to (Listmonk lets me manage multiple lists easily), or I can send test emails.\n\nThis is a solution I'm mostly happy with. I think there's still a bit of tweaking about some of the practicalities. I think there are two things I'm trying to figure out:\n\n1. Whether I should maintain separate lists for different categories of post. It wouldn't be hard to maintain category-specific lists so that people could easily opt-out of posts about (e.g.) how I built things on my website.\n2. How exactly to manage the sending of emails. When I sent out the last email, it only sent to like two-thirds of my subscribers because my email provider is Protonmail setup with a custom `blog@` email address for sending/receiving. Unfortunately, there were some rate limits that I didn't adequately respect the first time.\n\nI'm overall pretty pleased with this system.\n\n## Improvements in design and performance\n\nWithin me are two wolves: Sometimes I miss the days of HTML-only websites. Other days, I long for the ability to have nicely reactive and responsive design elements that require richer web frameworks. On some personal projects over the last few months, I've learned a bit more about modern webdev tools. One thing I'm particularly taken by is the way that modern webdev tools like vite manage javascript dependencies.\n\nIn short, the problem with a lot of these big web frameworks is that they're really big! They have a lot of stuff that you never use in a small personal website like mine, but they still get bundled into the library that is send down to the website viewer. We don't need to do that: we can be smarter. That's basically what tools like vite do. They only bundle together the parts of code that are necessary for the particular site being built. They also can avoid loading these libraries until they're actually needed.\n\nThat's all a bit of a tangent, but its gotten me thinking about why I actually need to load all of the Javascript packages that are being loaded with my website? Well, Quarto ultimately doesn't seem to be particularly optimized on this front. This is kind of the tradeoff you get: sure, it's easy to throw something together with Quarto that has a lot of the bells and whistled a data scientist craves, but that doesn't mean it's going to be super efficient. There's a [really nice post by Emil Hvitfeldt discussing performance considerations in Quarto](https://emilhvitfeldt.com/post/quarto-performance/).\n\n### Icons\n\nI in-lined many icons to embedded svgs, but, alas, Quarto still loads the `bootstrap-icons.css` that it does not, as far as I can tell, use anywhere.\n\n### Fonts\n\nHere's some fun Drew-lore: I'm weirdly into fonts. In the summer before my PhD I read like 3 books about fonts that I marked up more than almost anything else I've read. Long story short, I've currently landed on the following font styling that I intend to use pretty generally across the web, in documents and on presentations:\n\n- Headers in [URW Classico](). This is a font based largely on [Optima]().\n- Body text in [Domitian](). This is a font based largely on the venerable [Palatino]().\n- Code in [Monaspace Argon]().\n\nThese fonts are strongly humanist in orientation. Despite a brief flirtation with [Futura](), I have come around to the importance of fonts which communicate humanity. Ultimately, I think the omni-presence of sans-serifed fonts on the web is a reflection of its inhumanity. Good text should have character in style as well as in substance.\n\n## Remaining concerns\n\nI'm happy with a lot of what has come together.\n\n",
    "supporting": [
      "index_files/figure-html"
    ],
    "filters": [],
    "includes": {}
  }
}